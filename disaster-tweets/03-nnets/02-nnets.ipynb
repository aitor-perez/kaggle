{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### Loading data\n",
    "\n",
    "We start by importing the packages we are going to use:"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2021-09-13T15:08:44.744554Z",
     "iopub.execute_input": "2021-09-13T15:08:44.745573Z",
     "iopub.status.idle": "2021-09-13T15:08:44.756129Z",
     "shell.execute_reply.started": "2021-09-13T15:08:44.745483Z",
     "shell.execute_reply": "2021-09-13T15:08:44.754608Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/athena/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We load the datasets:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "train_data = pd.read_csv(\"../data/train.csv\")\n",
    "test_data = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "# We drop for now the keyword and location information\n",
    "train_data = train_data.drop(['id', 'keyword', 'location'], axis=1)\n",
    "test_data = test_data.drop(['keyword', 'location'], axis=1)\n",
    "\n",
    "train_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2021-09-13T15:08:54.835726Z",
     "iopub.execute_input": "2021-09-13T15:08:54.836899Z",
     "iopub.status.idle": "2021-09-13T15:08:54.893333Z",
     "shell.execute_reply.started": "2021-09-13T15:08:54.836857Z",
     "shell.execute_reply": "2021-09-13T15:08:54.89268Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   text  target\n0     Our Deeds are the Reason of this #earthquake M...       1\n1                Forest fire near La Ronge Sask. Canada       1\n2     All residents asked to 'shelter in place' are ...       1\n3     13,000 people receive #wildfires evacuation or...       1\n4     Just got sent this photo from Ruby #Alaska as ...       1\n...                                                 ...     ...\n7608  Two giant cranes holding a bridge collapse int...       1\n7609  @aria_ahrary @TheTawniest The out of control w...       1\n7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1\n7611  Police investigating after an e-bike collided ...       1\n7612  The Latest: More Homes Razed by Northern Calif...       1\n\n[7613 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7608</th>\n      <td>Two giant cranes holding a bridge collapse int...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7609</th>\n      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7610</th>\n      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7611</th>\n      <td>Police investigating after an e-bike collided ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7612</th>\n      <td>The Latest: More Homes Razed by Northern Calif...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>7613 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We clean the text by removing punctuation characters and stopwords:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def process_text(raw):\n",
    "    # Remove punctuation characters\n",
    "    no_punct = [char for char in raw if char not in string.punctuation]\n",
    "    no_punct = ''.join(no_punct)\n",
    "\n",
    "    # Remove stopwords\n",
    "    all_stopwords = stopwords.words('english') + ['u', 'ü', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n",
    "    no_stopwords = [word for word in no_punct.split() if word.lower() not in all_stopwords]\n",
    "    no_stopwords = ' '.join(no_stopwords)\n",
    "\n",
    "    return no_stopwords\n",
    "\n",
    "train_data['clean_text'] = train_data['text'].apply(process_text)\n",
    "test_data['clean_text'] = test_data['text'].apply(process_text)\n",
    "\n",
    "train_data"
   ],
   "metadata": {},
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/athena/opt/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                 text  target  \\\n0   Our Deeds are the Reason of this #earthquake M...       1   \n1              Forest fire near La Ronge Sask. Canada       1   \n2   All residents asked to 'shelter in place' are ...       1   \n3   13,000 people receive #wildfires evacuation or...       1   \n4   Just got sent this photo from Ruby #Alaska as ...       1   \n..                                                ...     ...   \n95  9 Mile backup on I-77 South...accident blockin...       1   \n96  Has an accident changed your life? We will hel...       0   \n97  #BREAKING: there was a deadly motorcycle car a...       1   \n98  @flowri were you marinading it or was it an ac...       0   \n99  only had a car for not even a week and got in ...       1   \n\n                                           clean_text  \n0        Deeds Reason earthquake May ALLAH Forgive us  \n1               Forest fire near La Ronge Sask Canada  \n2   residents asked shelter place notified officer...  \n3   13000 people receive wildfires evacuation orde...  \n4   got sent photo Ruby Alaska smoke wildfires pou...  \n..                                                ...  \n95  9 Mile backup I77 Southaccident blocking Right...  \n96  accident changed life help determine options f...  \n97  BREAKING deadly motorcycle car accident happen...  \n98                         flowri marinading accident  \n99  car even week got fucking car accident Mfs can...  \n\n[100 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n      <td>Deeds Reason earthquake May ALLAH Forgive us</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n      <td>Forest fire near La Ronge Sask Canada</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n      <td>residents asked shelter place notified officer...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n      <td>13000 people receive wildfires evacuation orde...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n      <td>got sent photo Ruby Alaska smoke wildfires pou...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>9 Mile backup on I-77 South...accident blockin...</td>\n      <td>1</td>\n      <td>9 Mile backup I77 Southaccident blocking Right...</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>Has an accident changed your life? We will hel...</td>\n      <td>0</td>\n      <td>accident changed life help determine options f...</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>#BREAKING: there was a deadly motorcycle car a...</td>\n      <td>1</td>\n      <td>BREAKING deadly motorcycle car accident happen...</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>@flowri were you marinading it or was it an ac...</td>\n      <td>0</td>\n      <td>flowri marinading accident</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>only had a car for not even a week and got in ...</td>\n      <td>1</td>\n      <td>car even week got fucking car accident Mfs can...</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We generate vector counts for both train and test data using scikit's **CountVectorizer**. In particular, notice that we fit the vectorizer only with the train tokens, and use it to transform both train and test data. If there are N unique tokens in the train dataset, for each tweet we obtain a vector of length N whose values are the word counts:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "cvec = CountVectorizer(stop_words='english')\n",
    "cvec.fit(train_data['clean_text'])\n",
    "X_train = cvec.transform(train_data['clean_text'])\n",
    "X_test = cvec.transform(test_data['clean_text'])\n",
    "y_train = train_data['target']\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2021-09-13T15:09:02.110714Z",
     "iopub.execute_input": "2021-09-13T15:09:02.111845Z",
     "iopub.status.idle": "2021-09-13T15:09:02.598749Z",
     "shell.execute_reply.started": "2021-09-13T15:09:02.111794Z",
     "shell.execute_reply": "2021-09-13T15:09:02.598066Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "((100, 553), (3263, 553), (100,))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Neural network\n",
    "\n",
    "We will train a model based on a neural network, using the **MLPClassifier** available in scikit-learn."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "nn = MLPClassifier(hidden_layer_sizes=(1000, 100), max_iter=10000, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2021-09-13T15:10:16.759668Z",
     "iopub.execute_input": "2021-09-13T15:10:16.759987Z",
     "iopub.status.idle": "2021-09-13T15:10:16.765146Z",
     "shell.execute_reply.started": "2021-09-13T15:10:16.759924Z",
     "shell.execute_reply": "2021-09-13T15:10:16.763711Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We train it with the whole train dataset:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "nn.fit(X_train, y_train)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-13T15:10:24.231996Z",
     "iopub.execute_input": "2021-09-13T15:10:24.232335Z",
     "iopub.status.idle": "2021-09-13T15:13:04.458918Z",
     "shell.execute_reply.started": "2021-09-13T15:10:24.232303Z",
     "shell.execute_reply": "2021-09-13T15:13:04.4579Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69664965\n",
      "Iteration 2, loss = 0.62496169\n",
      "Iteration 3, loss = 0.56261060\n",
      "Iteration 4, loss = 0.50296276\n",
      "Iteration 5, loss = 0.44449089\n",
      "Iteration 6, loss = 0.38694254\n",
      "Iteration 7, loss = 0.33130532\n",
      "Iteration 8, loss = 0.27858193\n",
      "Iteration 9, loss = 0.22998522\n",
      "Iteration 10, loss = 0.18669301\n",
      "Iteration 11, loss = 0.14938047\n",
      "Iteration 12, loss = 0.11811356\n",
      "Iteration 13, loss = 0.09257406\n",
      "Iteration 14, loss = 0.07213433\n",
      "Iteration 15, loss = 0.05600717\n",
      "Iteration 16, loss = 0.04343694\n",
      "Iteration 17, loss = 0.03373616\n",
      "Iteration 18, loss = 0.02628152\n",
      "Iteration 19, loss = 0.02056079\n",
      "Iteration 20, loss = 0.01617013\n",
      "Iteration 21, loss = 0.01280048\n",
      "Iteration 22, loss = 0.01020577\n",
      "Iteration 23, loss = 0.00820645\n",
      "Iteration 24, loss = 0.00665982\n",
      "Iteration 25, loss = 0.00546005\n",
      "Iteration 26, loss = 0.00452414\n",
      "Iteration 27, loss = 0.00379103\n",
      "Iteration 28, loss = 0.00321322\n",
      "Iteration 29, loss = 0.00275552\n",
      "Iteration 30, loss = 0.00239032\n",
      "Iteration 31, loss = 0.00209712\n",
      "Iteration 32, loss = 0.00186006\n",
      "Iteration 33, loss = 0.00166684\n",
      "Iteration 34, loss = 0.00150845\n",
      "Iteration 35, loss = 0.00137752\n",
      "Iteration 36, loss = 0.00126860\n",
      "Iteration 37, loss = 0.00117743\n",
      "Iteration 38, loss = 0.00110057\n",
      "Iteration 39, loss = 0.00103533\n",
      "Iteration 40, loss = 0.00097959\n",
      "Iteration 41, loss = 0.00093171\n",
      "Iteration 42, loss = 0.00089026\n",
      "Iteration 43, loss = 0.00085421\n",
      "Iteration 44, loss = 0.00082272\n",
      "Iteration 45, loss = 0.00079500\n",
      "Iteration 46, loss = 0.00077053\n",
      "Iteration 47, loss = 0.00074876\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": "MLPClassifier(hidden_layer_sizes=(1000, 100), max_iter=10000, verbose=1)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "And we generate the predictions for submission:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "y_pred = nn.predict(X_test)\n",
    "\n",
    "output = pd.DataFrame({'id': test_data['id'], 'target': y_pred})\n",
    "output.to_csv('predictions/nnets.csv', index=False)\n",
    "print(\"Submission successfully saved!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2021-09-13T15:13:20.767477Z",
     "iopub.execute_input": "2021-09-13T15:13:20.767825Z",
     "iopub.status.idle": "2021-09-13T15:13:20.800558Z",
     "shell.execute_reply.started": "2021-09-13T15:13:20.767795Z",
     "shell.execute_reply": "2021-09-13T15:13:20.799077Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission successfully saved!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}