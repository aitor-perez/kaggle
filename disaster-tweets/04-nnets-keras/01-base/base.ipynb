{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 04 Neural Networks with keras - 01 Base approach\n",
    "\n",
    "In this notebook we implement an approach based on neural networks, using the library **keras** from **tensorflow** to predict whether the tweets refer to a real disaster or not. We establish a fixed architecture with two convolutional layers followed by a dense layer, eyeballing the hyperparameters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loading data\n",
    "\n",
    "We start by importing the packages we are going to use and loading the datasets:"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras.layers import TextVectorization\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "train_data = pd.read_csv(\"../../data/train.csv\")\n",
    "test_data = pd.read_csv(\"../../data/test.csv\")\n",
    "\n",
    "train_data['text'].replace('http:\\/\\/\\S*', 'urltoken', regex=True, inplace=True)\n",
    "test_data['text'].replace('http:\\/\\/\\S*', 'urltoken', regex=True, inplace=True)\n",
    "\n",
    "train_text, train_label = np.array(train_data['text']), np.array(train_data['target'])\n",
    "test_text = test_data['text']\n",
    "\n",
    "print(train_text.shape)\n",
    "print(train_label.shape)\n",
    "print(test_text.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2021-09-13T15:08:54.835726Z",
     "iopub.execute_input": "2021-09-13T15:08:54.836899Z",
     "iopub.status.idle": "2021-09-13T15:08:54.893333Z",
     "shell.execute_reply.started": "2021-09-13T15:08:54.836857Z",
     "shell.execute_reply": "2021-09-13T15:08:54.89268Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613,)\n",
      "(7613,)\n",
      "(3263,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We explore the training data. There average tweet has 15 words, and the longest one has 31:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "count    7613.000000\nmean       14.903586\nstd         5.732604\nmin         1.000000\n25%        11.000000\n50%        15.000000\n75%        19.000000\nmax        31.000000\ndtype: float64"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word counts\n",
    "pd.Series(np.array([len(text.split()) for text in train_text])).describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are 27736 unique words in all the tweets:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "27736"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique tokens among all tweets\n",
    "len(np.unique(np.array(' '.join(train_text).split())))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model building\n",
    "\n",
    "The following function will create and return a model with a fixed layer architecture whose hyperparameters are defined above.\n",
    "\n",
    "We start with a **TextVectorization** layer with usual standardization, followed by an **Embedding** layer. We then compose with two **Conv1D** layers and perform **GlobalMaxPooling1D**, and finish with a **Dense** layer. We include some **Dropout** layers in order to avoid overfitting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Base model\n",
    "max_features = 20000\n",
    "sequence_length = 500\n",
    "\n",
    "embedding_dim = 128\n",
    "\n",
    "dropout_rate = 0.5\n",
    "\n",
    "conv_filters = 128\n",
    "\n",
    "conv_kernel_size = 7\n",
    "conv_strides = 3\n",
    "\n",
    "dense_layer_size = 128\n",
    "\n",
    "def build_model():\n",
    "    # Inputs are text strings, then we vectorize them\n",
    "    inputs = keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
    "\n",
    "    vectorizer = TextVectorization(\n",
    "        standardize='lower_and_strip_punctuation',\n",
    "        max_tokens=max_features,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=sequence_length,\n",
    "    )\n",
    "    vectorizer.adapt(train_text)\n",
    "    x = vectorizer(inputs)\n",
    "\n",
    "    # We use Embedding to map the vectorized text onto a space of dimension embedding_dim\n",
    "    x = Embedding(max_features + 1, embedding_dim)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Conv1D + GlobalMaxPooling\n",
    "    x = Conv1D(conv_filters, conv_kernel_size, strides=conv_strides, activation='relu')(x)\n",
    "    x = Conv1D(conv_filters, conv_kernel_size, strides=conv_strides, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "    # Dense hidden layer\n",
    "    x = Dense(dense_layer_size, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "    model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model training\n",
    "\n",
    "We are now ready to train the model. We start by creating an instance and printing a summary:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text (InputLayer)            [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization_1 (TextVe (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 500, 128)          2560128   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 165, 128)          114816    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 53, 128)           114816    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,806,401\n",
      "Trainable params: 2,806,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use 10-fold cross-validation and train for 3 epochs:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "> Fold 1\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 39s - loss: 0.6361 - accuracy: 0.6260 - precision_2: 0.7154 - recall_2: 0.2130\n",
      "Epoch 2/3\n",
      "215/215 - 38s - loss: 0.3834 - accuracy: 0.8396 - precision_2: 0.8716 - recall_2: 0.7343\n",
      "Epoch 3/3\n",
      "215/215 - 34s - loss: 0.1776 - accuracy: 0.9365 - precision_2: 0.9494 - recall_2: 0.9000\n",
      "215/215 - 7s - loss: 0.0712 - accuracy: 0.9768 - precision_2: 0.9764 - recall_2: 0.9694\n",
      "24/24 - 1s - loss: 0.5756 - accuracy: 0.7717 - precision_2: 0.7257 - recall_2: 0.7651\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 42s - loss: 0.6279 - accuracy: 0.6417 - precision_3: 0.6974 - recall_3: 0.2974\n",
      "Epoch 2/3\n",
      "215/215 - 37s - loss: 0.3630 - accuracy: 0.8542 - precision_3: 0.8766 - recall_3: 0.7700\n",
      "Epoch 3/3\n",
      "215/215 - 35s - loss: 0.1666 - accuracy: 0.9396 - precision_3: 0.9471 - recall_3: 0.9106\n",
      "215/215 - 6s - loss: 0.0682 - accuracy: 0.9790 - precision_3: 0.9825 - recall_3: 0.9685\n",
      "24/24 - 0s - loss: 0.5665 - accuracy: 0.7822 - precision_3: 0.7508 - recall_3: 0.7179\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 32s - loss: 0.6473 - accuracy: 0.6187 - precision_4: 0.6772 - recall_4: 0.2114\n",
      "Epoch 2/3\n",
      "215/215 - 36s - loss: 0.3962 - accuracy: 0.8383 - precision_4: 0.8574 - recall_4: 0.7470\n",
      "Epoch 3/3\n",
      "215/215 - 33s - loss: 0.1842 - accuracy: 0.9353 - precision_4: 0.9450 - recall_4: 0.9016\n",
      "215/215 - 6s - loss: 0.0733 - accuracy: 0.9769 - precision_4: 0.9655 - recall_4: 0.9813\n",
      "24/24 - 0s - loss: 0.6266 - accuracy: 0.7362 - precision_4: 0.6727 - recall_4: 0.7754\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 34s - loss: 0.6299 - accuracy: 0.6404 - precision_5: 0.7391 - recall_5: 0.2440\n",
      "Epoch 2/3\n",
      "215/215 - 22s - loss: 0.3772 - accuracy: 0.8441 - precision_5: 0.8595 - recall_5: 0.7591\n",
      "Epoch 3/3\n",
      "215/215 - 28s - loss: 0.1719 - accuracy: 0.9380 - precision_5: 0.9439 - recall_5: 0.9087\n",
      "215/215 - 5s - loss: 0.0620 - accuracy: 0.9787 - precision_5: 0.9806 - recall_5: 0.9692\n",
      "24/24 - 1s - loss: 0.5490 - accuracy: 0.7937 - precision_5: 0.7781 - recall_5: 0.7623\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 37s - loss: 0.6416 - accuracy: 0.6211 - precision_6: 0.6769 - recall_6: 0.2313\n",
      "Epoch 2/3\n",
      "215/215 - 34s - loss: 0.3741 - accuracy: 0.8463 - precision_6: 0.8691 - recall_6: 0.7575\n",
      "Epoch 3/3\n",
      "215/215 - 33s - loss: 0.1831 - accuracy: 0.9364 - precision_6: 0.9490 - recall_6: 0.9008\n",
      "215/215 - 6s - loss: 0.0720 - accuracy: 0.9772 - precision_6: 0.9864 - recall_6: 0.9604\n",
      "24/24 - 1s - loss: 0.5856 - accuracy: 0.7727 - precision_6: 0.7287 - recall_6: 0.7264\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 32s - loss: 0.6419 - accuracy: 0.6332 - precision_7: 0.6975 - recall_7: 0.2544\n",
      "Epoch 2/3\n",
      "215/215 - 34s - loss: 0.3851 - accuracy: 0.8393 - precision_7: 0.8514 - recall_7: 0.7572\n",
      "Epoch 3/3\n",
      "215/215 - 35s - loss: 0.1843 - accuracy: 0.9365 - precision_7: 0.9405 - recall_7: 0.9094\n",
      "215/215 - 6s - loss: 0.0897 - accuracy: 0.9748 - precision_7: 0.9929 - recall_7: 0.9479\n",
      "24/24 - 1s - loss: 0.5459 - accuracy: 0.7832 - precision_7: 0.8696 - recall_7: 0.5970\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 36s - loss: 0.6435 - accuracy: 0.6223 - precision_8: 0.7099 - recall_8: 0.2175\n",
      "Epoch 2/3\n",
      "215/215 - 34s - loss: 0.3810 - accuracy: 0.8427 - precision_8: 0.8710 - recall_8: 0.7478\n",
      "Epoch 3/3\n",
      "215/215 - 34s - loss: 0.1861 - accuracy: 0.9314 - precision_8: 0.9433 - recall_8: 0.8956\n",
      "215/215 - 5s - loss: 0.0712 - accuracy: 0.9783 - precision_8: 0.9796 - recall_8: 0.9700\n",
      "24/24 - 0s - loss: 0.5598 - accuracy: 0.7740 - precision_8: 0.7022 - recall_8: 0.7442\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 33s - loss: 0.6140 - accuracy: 0.6503 - precision_9: 0.6747 - recall_9: 0.3594\n",
      "Epoch 2/3\n",
      "215/215 - 31s - loss: 0.3621 - accuracy: 0.8525 - precision_9: 0.8710 - recall_9: 0.7707\n",
      "Epoch 3/3\n",
      "215/215 - 35s - loss: 0.1665 - accuracy: 0.9466 - precision_9: 0.9555 - recall_9: 0.9185\n",
      "215/215 - 5s - loss: 0.0664 - accuracy: 0.9784 - precision_9: 0.9878 - recall_9: 0.9616\n",
      "24/24 - 1s - loss: 0.5168 - accuracy: 0.8003 - precision_9: 0.8028 - recall_9: 0.7095\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 35s - loss: 0.6294 - accuracy: 0.6512 - precision_10: 0.7384 - recall_10: 0.2837\n",
      "Epoch 2/3\n",
      "215/215 - 32s - loss: 0.3536 - accuracy: 0.8533 - precision_10: 0.8710 - recall_10: 0.7707\n",
      "Epoch 3/3\n",
      "215/215 - 26s - loss: 0.1599 - accuracy: 0.9461 - precision_10: 0.9516 - recall_10: 0.9207\n",
      "215/215 - 4s - loss: 0.0645 - accuracy: 0.9802 - precision_10: 0.9774 - recall_10: 0.9761\n",
      "24/24 - 1s - loss: 0.5829 - accuracy: 0.7753 - precision_10: 0.7719 - recall_10: 0.7159\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 27s - loss: 0.6394 - accuracy: 0.6302 - precision_11: 0.6724 - recall_11: 0.2784\n",
      "Epoch 2/3\n",
      "215/215 - 26s - loss: 0.3796 - accuracy: 0.8435 - precision_11: 0.8587 - recall_11: 0.7629\n",
      "Epoch 3/3\n",
      "215/215 - 27s - loss: 0.1722 - accuracy: 0.9410 - precision_11: 0.9502 - recall_11: 0.9110\n",
      "215/215 - 5s - loss: 0.0766 - accuracy: 0.9797 - precision_11: 0.9754 - recall_11: 0.9777\n",
      "24/24 - 1s - loss: 0.6235 - accuracy: 0.7411 - precision_11: 0.6648 - recall_11: 0.7556\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "scores = []\n",
    "models = []\n",
    "\n",
    "i = 1\n",
    "for fold_train_indices, fold_val_indices in kfold.split(train_text, train_label):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i}')\n",
    "    print('------------------------------------------------------------------------')\n",
    "\n",
    "    fold_train_text = train_text[fold_train_indices]\n",
    "    fold_train_label = train_label[fold_train_indices]\n",
    "    fold_val_text = train_text[fold_val_indices]\n",
    "    fold_val_label = train_label[fold_val_indices]\n",
    "\n",
    "    model = build_model()\n",
    "    model.fit(fold_train_text, fold_train_label, epochs=epochs, verbose=2)\n",
    "    models.append(model)\n",
    "\n",
    "    fold_train_score = model.evaluate(fold_train_text, fold_train_label, verbose=2)\n",
    "    fold_val_score = model.evaluate(fold_val_text, fold_val_label, verbose=2)\n",
    "    scores.append({'train': fold_train_score, 'val': fold_val_score})\n",
    "\n",
    "    i += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We compute the F1-score:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "for fold_scores in scores:\n",
    "    for subset in ['train', 'val']:\n",
    "        precision = fold_scores[subset][2]\n",
    "        recall = fold_scores[subset][3]\n",
    "        f1_score = 2/(1/precision + 1/recall)\n",
    "        fold_scores[subset].append(f1_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And print a detailed summary of the scores:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Average scores for all folds - Train\n",
      "> Loss: 0.0715 -  Accuracy: 0.978 - Precision: 0.9804 - Recall: 0.9682 - F1-score: 0.9742\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds - Validation\n",
      "> Loss: 0.5732 -  Accuracy: 0.773 - Precision: 0.7467 - Recall: 0.7269 - F1-score: 0.7331\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Train\n",
      ">>> Loss: 0.0712 - Accuracy: 0.9768 - Precision: 0.9764 - Recall: 0.9694 - F1-score: 0.9729\n",
      "> Fold 1 - Validation\n",
      ">>> Loss: 0.5756 - Accuracy: 0.7717 - Precision: 0.7257 - Recall: 0.7651 - F1-score: 0.7449\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Train\n",
      ">>> Loss: 0.0682 - Accuracy: 0.979 - Precision: 0.9825 - Recall: 0.9685 - F1-score: 0.9754\n",
      "> Fold 2 - Validation\n",
      ">>> Loss: 0.5665 - Accuracy: 0.7822 - Precision: 0.7508 - Recall: 0.7179 - F1-score: 0.734\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Train\n",
      ">>> Loss: 0.0733 - Accuracy: 0.9769 - Precision: 0.9655 - Recall: 0.9813 - F1-score: 0.9733\n",
      "> Fold 3 - Validation\n",
      ">>> Loss: 0.6266 - Accuracy: 0.7362 - Precision: 0.6727 - Recall: 0.7754 - F1-score: 0.7204\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Train\n",
      ">>> Loss: 0.062 - Accuracy: 0.9787 - Precision: 0.9806 - Recall: 0.9692 - F1-score: 0.9749\n",
      "> Fold 4 - Validation\n",
      ">>> Loss: 0.549 - Accuracy: 0.7937 - Precision: 0.7781 - Recall: 0.7623 - F1-score: 0.7701\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Train\n",
      ">>> Loss: 0.072 - Accuracy: 0.9772 - Precision: 0.9864 - Recall: 0.9604 - F1-score: 0.9732\n",
      "> Fold 5 - Validation\n",
      ">>> Loss: 0.5856 - Accuracy: 0.7727 - Precision: 0.7287 - Recall: 0.7264 - F1-score: 0.7276\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Train\n",
      ">>> Loss: 0.0897 - Accuracy: 0.9748 - Precision: 0.9929 - Recall: 0.9479 - F1-score: 0.9699\n",
      "> Fold 6 - Validation\n",
      ">>> Loss: 0.5459 - Accuracy: 0.7832 - Precision: 0.8696 - Recall: 0.597 - F1-score: 0.708\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Train\n",
      ">>> Loss: 0.0712 - Accuracy: 0.9783 - Precision: 0.9796 - Recall: 0.97 - F1-score: 0.9748\n",
      "> Fold 7 - Validation\n",
      ">>> Loss: 0.5598 - Accuracy: 0.774 - Precision: 0.7022 - Recall: 0.7442 - F1-score: 0.7226\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Train\n",
      ">>> Loss: 0.0664 - Accuracy: 0.9784 - Precision: 0.9878 - Recall: 0.9616 - F1-score: 0.9745\n",
      "> Fold 8 - Validation\n",
      ">>> Loss: 0.5168 - Accuracy: 0.8003 - Precision: 0.8028 - Recall: 0.7095 - F1-score: 0.7532\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Train\n",
      ">>> Loss: 0.0645 - Accuracy: 0.9802 - Precision: 0.9774 - Recall: 0.9761 - F1-score: 0.9767\n",
      "> Fold 9 - Validation\n",
      ">>> Loss: 0.5829 - Accuracy: 0.7753 - Precision: 0.7719 - Recall: 0.7159 - F1-score: 0.7429\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Train\n",
      ">>> Loss: 0.0766 - Accuracy: 0.9797 - Precision: 0.9754 - Recall: 0.9777 - F1-score: 0.9765\n",
      "> Fold 10 - Validation\n",
      ">>> Loss: 0.6235 - Accuracy: 0.7411 - Precision: 0.6648 - Recall: 0.7556 - F1-score: 0.7073\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds - Train')\n",
    "print(f'> Loss: {round(np.mean([fold_score[\"train\"][0] for fold_score in scores]), 4)} -  Accuracy: {round(np.mean([fold_score[\"train\"][1] for fold_score in scores]), 4)} - Precision: {round(np.mean([fold_score[\"train\"][2] for fold_score in scores]), 4)} - Recall: {round(np.mean([fold_score[\"train\"][3] for fold_score in scores]), 4)} - F1-score: {round(np.mean([fold_score[\"train\"][4] for fold_score in scores]), 4)}')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds - Validation')\n",
    "print(f'> Loss: {round(np.mean([fold_score[\"val\"][0] for fold_score in scores]), 4)} -  Accuracy: {round(np.mean([fold_score[\"val\"][1] for fold_score in scores]), 4)} - Precision: {round(np.mean([fold_score[\"val\"][2] for fold_score in scores]), 4)} - Recall: {round(np.mean([fold_score[\"val\"][3] for fold_score in scores]), 4)} - F1-score: {round(np.mean([fold_score[\"val\"][4] for fold_score in scores]), 4)}')\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "i = 1\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for fold_scores in scores:\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i} - Train')\n",
    "    print(f'>>> Loss: {round(fold_scores[\"train\"][0], 4)} - Accuracy: {round(fold_scores[\"train\"][1], 4)} - Precision: {round(fold_scores[\"train\"][2], 4)} - Recall: {round(fold_scores[\"train\"][3], 4)} - F1-score: {round(fold_scores[\"train\"][4], 4)}')\n",
    "    print(f'> Fold {i} - Validation')\n",
    "    print(f'>>> Loss: {round(fold_scores[\"val\"][0], 4)} - Accuracy: {round(fold_scores[\"val\"][1], 4)} - Precision: {round(fold_scores[\"val\"][2], 4)} - Recall: {round(fold_scores[\"val\"][3], 4)} - F1-score: {round(fold_scores[\"val\"][4], 4)}')\n",
    "    i += 1\n",
    "print('------------------------------------------------------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Submission\n",
    "\n",
    "We take the model and train it with all the available data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "238/238 - 31s - loss: 0.6142 - accuracy: 0.6559 - precision_12: 0.7479 - recall_12: 0.3002\n",
      "Epoch 2/3\n",
      "238/238 - 28s - loss: 0.3579 - accuracy: 0.8528 - precision_12: 0.8788 - recall_12: 0.7625\n",
      "Epoch 3/3\n",
      "238/238 - 30s - loss: 0.1727 - accuracy: 0.9384 - precision_12: 0.9482 - recall_12: 0.9061\n",
      "238/238 - 5s - loss: 0.0657 - accuracy: 0.9786 - precision_12: 0.9796 - recall_12: 0.9703\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.06569287925958633,\n 0.9785892367362976,\n 0.979629635810852,\n 0.9703454375267029]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.fit(train_text, train_label, epochs=epochs, verbose=2)\n",
    "\n",
    "model.evaluate(train_text, train_label, verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We generate the predictions for the test set:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 1, 1, ..., 1, 1, 1])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict(test_text)\n",
    "test_pred = np.round(test_pred).flatten().astype('int')\n",
    "\n",
    "test_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And we save the predictions into a csv file ready for submission:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "output = pd.DataFrame({'id': test_data['id'], 'target': test_pred})\n",
    "output.to_csv('predictions/nnets.csv', index=False)\n",
    "print(\"Submission successfully saved!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2021-09-13T15:13:20.767477Z",
     "iopub.execute_input": "2021-09-13T15:13:20.767825Z",
     "iopub.status.idle": "2021-09-13T15:13:20.800558Z",
     "shell.execute_reply.started": "2021-09-13T15:13:20.767795Z",
     "shell.execute_reply": "2021-09-13T15:13:20.799077Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission successfully saved!\n"
     ]
    }
   ]
  }
 ]
}