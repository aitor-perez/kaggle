{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 04 - Neural Networks with keras\n",
    "\n",
    "In this notebook we implement an approach based on neural networks, using the library **keras** from **tensorflow** to predict whether the tweets refer to a real disaster or not."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loading data\n",
    "\n",
    "We start by importing the packages we are going to use and loading the datasets:"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras.layers import TextVectorization\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data = pd.read_csv(\"../data/train.csv\")\n",
    "test_data = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "train_text, train_label = np.array(train_data['text']), np.array(train_data['target'])\n",
    "test_text = test_data['text']\n",
    "\n",
    "print(train_text.shape)\n",
    "print(train_label.shape)\n",
    "print(test_text.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2021-09-13T15:08:54.835726Z",
     "iopub.execute_input": "2021-09-13T15:08:54.836899Z",
     "iopub.status.idle": "2021-09-13T15:08:54.893333Z",
     "shell.execute_reply.started": "2021-09-13T15:08:54.836857Z",
     "shell.execute_reply": "2021-09-13T15:08:54.89268Z"
    },
    "trusted": true
   },
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613,)\n",
      "(7613,)\n",
      "(3263,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "sequence_length = 500\n",
    "\n",
    "embedding_dim = 128\n",
    "\n",
    "dropout_rate = 0.5\n",
    "\n",
    "conv_filters = 128\n",
    "conv_kernel_size = 7\n",
    "conv_strides = 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We clean the text by removing punctuation characters and stopwords:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def custom_standardization(raw):\n",
    "    lowercase = tf.strings.lower(raw)\n",
    "\n",
    "    no_punct = tf.strings.regex_replace(\n",
    "        lowercase, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
    "    )\n",
    "\n",
    "    return no_punct\n",
    "\n",
    "vectorizer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "vectorizer.adapt(train_text)"
   ],
   "metadata": {},
   "execution_count": 109,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Inputs are text strings, then we vectorize them\n",
    "    inputs = keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
    "    x = vectorizer(inputs)\n",
    "\n",
    "    # We use Embedding to map the vectorized text onto a space of dimension embedding_dim\n",
    "    x = Embedding(max_features + 1, embedding_dim)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Conv1D + GlobalMaxPooling\n",
    "    x = Conv1D(conv_filters, conv_kernel_size, strides=conv_strides, activation='relu')(x)\n",
    "    x = Conv1D(conv_filters, conv_kernel_size, strides=conv_strides, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "    # Dense hidden layer\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 1/3\n",
      "215/215 - 31s - loss: 0.6124 - accuracy: 0.6564\n",
      "Epoch 2/3\n",
      "215/215 - 28s - loss: 0.3559 - accuracy: 0.8568\n",
      "Epoch 3/3\n",
      "215/215 - 28s - loss: 0.1605 - accuracy: 0.9435\n",
      "24/24 - 1s - loss: 0.5471 - accuracy: 0.7690\n",
      "**********\n",
      "Epoch 1/3\n",
      "215/215 - 29s - loss: 0.6397 - accuracy: 0.6259\n",
      "Epoch 2/3\n",
      "215/215 - 28s - loss: 0.3721 - accuracy: 0.8448\n",
      "Epoch 3/3\n",
      "215/215 - 28s - loss: 0.1640 - accuracy: 0.9431\n",
      "24/24 - 1s - loss: 0.6261 - accuracy: 0.7769\n",
      "**********\n",
      "Epoch 1/3\n",
      "215/215 - 29s - loss: 0.6221 - accuracy: 0.6455\n",
      "Epoch 2/3\n",
      "215/215 - 28s - loss: 0.3574 - accuracy: 0.8501\n",
      "Epoch 3/3\n",
      "215/215 - 28s - loss: 0.1610 - accuracy: 0.9423\n",
      "24/24 - 1s - loss: 0.6448 - accuracy: 0.7638\n",
      "**********\n",
      "Epoch 1/3\n",
      "215/215 - 31s - loss: 0.6329 - accuracy: 0.6293\n",
      "Epoch 2/3\n",
      "215/215 - 28s - loss: 0.3775 - accuracy: 0.8473\n",
      "Epoch 3/3\n",
      "215/215 - 28s - loss: 0.1628 - accuracy: 0.9431\n",
      "24/24 - 1s - loss: 0.5941 - accuracy: 0.7595\n",
      "**********\n",
      "Epoch 1/3\n",
      "215/215 - 29s - loss: 0.6380 - accuracy: 0.6267\n",
      "Epoch 2/3\n",
      "215/215 - 28s - loss: 0.3817 - accuracy: 0.8424\n",
      "Epoch 3/3\n",
      "215/215 - 28s - loss: 0.1672 - accuracy: 0.9425\n",
      "24/24 - 1s - loss: 0.5131 - accuracy: 0.8147\n",
      "**********\n",
      "Epoch 1/3\n",
      "215/215 - 29s - loss: 0.6251 - accuracy: 0.6448\n",
      "Epoch 2/3\n",
      "215/215 - 27s - loss: 0.3572 - accuracy: 0.8539\n",
      "Epoch 3/3\n",
      "215/215 - 27s - loss: 0.1638 - accuracy: 0.9409\n",
      "24/24 - 1s - loss: 0.6829 - accuracy: 0.7280\n",
      "**********\n",
      "Epoch 1/3\n",
      "215/215 - 29s - loss: 0.6398 - accuracy: 0.6277\n",
      "Epoch 2/3\n",
      "215/215 - 27s - loss: 0.3799 - accuracy: 0.8400\n",
      "Epoch 3/3\n",
      "215/215 - 29s - loss: 0.1843 - accuracy: 0.9340\n",
      "24/24 - 1s - loss: 0.5642 - accuracy: 0.7898\n",
      "**********\n",
      "Epoch 1/3\n",
      "215/215 - 29s - loss: 0.6472 - accuracy: 0.6181\n",
      "Epoch 2/3\n",
      "215/215 - 28s - loss: 0.3933 - accuracy: 0.8338\n",
      "Epoch 3/3\n",
      "215/215 - 27s - loss: 0.1759 - accuracy: 0.9378\n",
      "24/24 - 1s - loss: 0.6322 - accuracy: 0.7661\n",
      "**********\n",
      "Epoch 1/3\n",
      "215/215 - 29s - loss: 0.6455 - accuracy: 0.6213\n",
      "Epoch 2/3\n",
      "215/215 - 28s - loss: 0.3810 - accuracy: 0.8427\n",
      "Epoch 3/3\n",
      "215/215 - 27s - loss: 0.1676 - accuracy: 0.9412\n",
      "24/24 - 1s - loss: 0.5695 - accuracy: 0.7700\n",
      "**********\n",
      "Epoch 1/3\n",
      "215/215 - 29s - loss: 0.6288 - accuracy: 0.6384\n",
      "Epoch 2/3\n",
      "215/215 - 27s - loss: 0.3730 - accuracy: 0.8355\n",
      "Epoch 3/3\n",
      "215/215 - 28s - loss: 0.1746 - accuracy: 0.9355\n",
      "24/24 - 1s - loss: 0.5331 - accuracy: 0.7845\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "kfold_losses = []\n",
    "kfold_accuracies = []\n",
    "\n",
    "for fold_train_indices, fold_val_indices in kfold.split(train_text, train_label):\n",
    "    print('*'*10)\n",
    "    fold_train_text = train_text[fold_train_indices]\n",
    "    fold_train_label = train_label[fold_train_indices]\n",
    "    fold_val_text = train_text[fold_val_indices]\n",
    "    fold_val_label = train_label[fold_val_indices]\n",
    "\n",
    "    model = build_model()\n",
    "\n",
    "    model.fit(fold_train_text, fold_train_label, epochs=epochs, verbose=2)\n",
    "\n",
    "    scores = model.evaluate(fold_val_text, fold_val_label, verbose=2)\n",
    "\n",
    "    kfold_losses.append(scores[0])\n",
    "    kfold_accuracies.append(scores[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.547137975692749 - Accuracy: 0.7690288424491882%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6261386275291443 - Accuracy: 0.7769029140472412%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6447669267654419 - Accuracy: 0.7637795209884644%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.5940704941749573 - Accuracy: 0.7595269680023193%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5130724906921387 - Accuracy: 0.8147174715995789%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.6829480528831482 - Accuracy: 0.7279894948005676%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.5642150640487671 - Accuracy: 0.789750337600708%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.6322306394577026 - Accuracy: 0.7660972476005554%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.5694745182991028 - Accuracy: 0.770039439201355%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.5331341624259949 - Accuracy: 0.7844941020011902%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 0.7722326338291168 (+- 0.021276861288300548)\n",
      "> Loss: 0.5907188951969147\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(kfold_losses)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {kfold_losses[i]} - Accuracy: {kfold_accuracies[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(kfold_accuracies)} (+- {np.std(kfold_accuracies)})')\n",
    "print(f'> Loss: {np.mean(kfold_losses)}')\n",
    "print('------------------------------------------------------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 - 7s - loss: 0.0064 - accuracy: 0.9959\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.006378879304975271, 0.9959279894828796]"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train_text, train_label, verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 1, 1, ..., 0, 1, 1])"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict(test_text)\n",
    "test_pred = np.round(test_pred).flatten().astype('int')\n",
    "\n",
    "test_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We generate vector counts for both train and test data using scikit's **CountVectorizer**. In particular, notice that we fit the vectorizer only with the train tokens, and use it to transform both train and test data. If there are N unique tokens in the train dataset, for each tweet we obtain a vector of length N whose values are the word counts:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "output = pd.DataFrame({'id': test_data['id'], 'target': test_pred})\n",
    "output.to_csv('predictions/nnets.csv', index=False)\n",
    "print(\"Submission successfully saved!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2021-09-13T15:13:20.767477Z",
     "iopub.execute_input": "2021-09-13T15:13:20.767825Z",
     "iopub.status.idle": "2021-09-13T15:13:20.800558Z",
     "shell.execute_reply.started": "2021-09-13T15:13:20.767795Z",
     "shell.execute_reply": "2021-09-13T15:13:20.799077Z"
    },
    "trusted": true
   },
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission successfully saved!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}