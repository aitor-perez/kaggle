{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 04 - Neural Networks with keras\n",
    "\n",
    "In this notebook we implement an approach based on neural networks, using the library **keras** from **tensorflow** to predict whether the tweets refer to a real disaster or not."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loading data\n",
    "\n",
    "We start by importing the packages we are going to use and loading the datasets:"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras.layers import TextVectorization\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "train_data = pd.read_csv(\"../data/train.csv\")\n",
    "test_data = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "train_text, train_label = np.array(train_data['text']), np.array(train_data['target'])\n",
    "test_text = test_data['text']\n",
    "\n",
    "print(train_text.shape)\n",
    "print(train_label.shape)\n",
    "print(test_text.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2021-09-13T15:08:54.835726Z",
     "iopub.execute_input": "2021-09-13T15:08:54.836899Z",
     "iopub.status.idle": "2021-09-13T15:08:54.893333Z",
     "shell.execute_reply.started": "2021-09-13T15:08:54.836857Z",
     "shell.execute_reply": "2021-09-13T15:08:54.89268Z"
    },
    "trusted": true
   },
   "execution_count": 270,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613,)\n",
      "(7613,)\n",
      "(3263,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [
    {
     "data": {
      "text/plain": "count    7613.000000\nmean       14.903586\nstd         5.732604\nmin         1.000000\n25%        11.000000\n50%        15.000000\n75%        19.000000\nmax        31.000000\ndtype: float64"
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.array([len(text.split()) for text in train_text])).describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "outputs": [
    {
     "data": {
      "text/plain": "31924"
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(np.array(' '.join(train_text).split())))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [],
   "source": [
    "# max_features = 20000\n",
    "max_features = 10000\n",
    "\n",
    "sequence_length = 500\n",
    "# sequence_length = 32\n",
    "\n",
    "embedding_dim = 128\n",
    "# embedding_dim = 64\n",
    "\n",
    "dropout_rate = 0.5\n",
    "\n",
    "conv_filters = 128\n",
    "# conv_filters = 64\n",
    "conv_kernel_size = 7\n",
    "conv_strides = 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We clean the text by removing punctuation characters and stopwords:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def custom_standardization(raw):\n",
    "    lowercase = tf.strings.lower(raw)\n",
    "\n",
    "    no_punct = tf.strings.regex_replace(\n",
    "        lowercase, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
    "    )\n",
    "\n",
    "    return no_punct\n",
    "\n",
    "vectorizer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "vectorizer.adapt(train_text)\n",
    "\n",
    "len(vectorizer.get_vocabulary())"
   ],
   "metadata": {},
   "execution_count": 274,
   "outputs": [
    {
     "data": {
      "text/plain": "10000"
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Inputs are text strings, then we vectorize them\n",
    "    inputs = keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
    "    x = vectorizer(inputs)\n",
    "\n",
    "    # We use Embedding to map the vectorized text onto a space of dimension embedding_dim\n",
    "    x = Embedding(max_features + 1, embedding_dim)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Conv1D + GlobalMaxPooling\n",
    "    x = Conv1D(conv_filters, conv_kernel_size, strides=conv_strides, activation='relu')(x)\n",
    "    x = Conv1D(conv_filters, conv_kernel_size, strides=conv_strides, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "    # Dense hidden layer\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "    model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "model = build_model()\n",
    "model.fit(train_text, train_label, epochs=epochs, verbose=2)\n",
    "\n",
    "model.evaluate(train_text, train_label, verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "> Fold 1\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 50s - loss: 0.6403 - accuracy: 0.6257 - precision_84: 0.6928 - recall_84: 0.2414\n",
      "Epoch 2/3\n",
      "215/215 - 33s - loss: 0.3908 - accuracy: 0.8358 - precision_84: 0.8470 - recall_84: 0.7569\n",
      "Epoch 3/3\n",
      "215/215 - 43s - loss: 0.2141 - accuracy: 0.9223 - precision_84: 0.9336 - recall_84: 0.8832\n",
      "215/215 - 7s - loss: 0.0934 - accuracy: 0.9680 - precision_84: 0.9814 - recall_84: 0.9440\n",
      "24/24 - 1s - loss: 0.5739 - accuracy: 0.7861 - precision_84: 0.7466 - recall_84: 0.7152\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 44s - loss: 0.6291 - accuracy: 0.6341 - precision_85: 0.7172 - recall_85: 0.2484\n",
      "Epoch 2/3\n",
      "215/215 - 40s - loss: 0.3924 - accuracy: 0.8351 - precision_85: 0.8697 - recall_85: 0.7259\n",
      "Epoch 3/3\n",
      "215/215 - 42s - loss: 0.2224 - accuracy: 0.9174 - precision_85: 0.9291 - recall_85: 0.8750\n",
      "215/215 - 9s - loss: 0.0993 - accuracy: 0.9658 - precision_85: 0.9888 - recall_85: 0.9312\n",
      "24/24 - 1s - loss: 0.4985 - accuracy: 0.7992 - precision_85: 0.8059 - recall_85: 0.6875\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 49s - loss: 0.6351 - accuracy: 0.6256 - precision_86: 0.7090 - recall_86: 0.2163\n",
      "Epoch 2/3\n",
      "215/215 - 39s - loss: 0.3900 - accuracy: 0.8412 - precision_86: 0.8657 - recall_86: 0.7456\n",
      "Epoch 3/3\n",
      "215/215 - 31s - loss: 0.2112 - accuracy: 0.9221 - precision_86: 0.9327 - recall_86: 0.8820\n",
      "215/215 - 6s - loss: 0.0883 - accuracy: 0.9680 - precision_86: 0.9789 - recall_86: 0.9459\n",
      "24/24 - 1s - loss: 0.5747 - accuracy: 0.7861 - precision_86: 0.8000 - recall_86: 0.6767\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 32s - loss: 0.6455 - accuracy: 0.6214 - precision_87: 0.7000 - recall_87: 0.2112\n",
      "Epoch 2/3\n",
      "215/215 - 31s - loss: 0.4041 - accuracy: 0.8300 - precision_87: 0.8377 - recall_87: 0.7505\n",
      "Epoch 3/3\n",
      "215/215 - 30s - loss: 0.2301 - accuracy: 0.9161 - precision_87: 0.9289 - recall_87: 0.8719\n",
      "215/215 - 6s - loss: 0.1105 - accuracy: 0.9656 - precision_87: 0.9761 - recall_87: 0.9431\n",
      "24/24 - 1s - loss: 0.5641 - accuracy: 0.7806 - precision_87: 0.7831 - recall_87: 0.6636\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 31s - loss: 0.6215 - accuracy: 0.6493 - precision_88: 0.7408 - recall_88: 0.2745\n",
      "Epoch 2/3\n",
      "215/215 - 30s - loss: 0.3782 - accuracy: 0.8452 - precision_88: 0.8678 - recall_88: 0.7518\n",
      "Epoch 3/3\n",
      "215/215 - 30s - loss: 0.2049 - accuracy: 0.9253 - precision_88: 0.9357 - recall_88: 0.8858\n",
      "215/215 - 6s - loss: 0.0990 - accuracy: 0.9730 - precision_88: 0.9814 - recall_88: 0.9549\n",
      "24/24 - 1s - loss: 0.5516 - accuracy: 0.7674 - precision_88: 0.7700 - recall_88: 0.6965\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 32s - loss: 0.6257 - accuracy: 0.6400 - precision_89: 0.7160 - recall_89: 0.2609\n",
      "Epoch 2/3\n",
      "215/215 - 30s - loss: 0.3834 - accuracy: 0.8389 - precision_89: 0.8602 - recall_89: 0.7439\n",
      "Epoch 3/3\n",
      "215/215 - 30s - loss: 0.2156 - accuracy: 0.9219 - precision_89: 0.9375 - recall_89: 0.8757\n",
      "215/215 - 5s - loss: 0.0909 - accuracy: 0.9723 - precision_89: 0.9848 - recall_89: 0.9498\n",
      "24/24 - 1s - loss: 0.6019 - accuracy: 0.7635 - precision_89: 0.7726 - recall_89: 0.6735\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 31s - loss: 0.6300 - accuracy: 0.6375 - precision_90: 0.7051 - recall_90: 0.2715\n",
      "Epoch 2/3\n",
      "215/215 - 30s - loss: 0.3821 - accuracy: 0.8363 - precision_90: 0.8641 - recall_90: 0.7353\n",
      "Epoch 3/3\n",
      "215/215 - 31s - loss: 0.2170 - accuracy: 0.9151 - precision_90: 0.9305 - recall_90: 0.8675\n",
      "215/215 - 5s - loss: 0.1014 - accuracy: 0.9712 - precision_90: 0.9781 - recall_90: 0.9546\n",
      "24/24 - 1s - loss: 0.5852 - accuracy: 0.7608 - precision_90: 0.7100 - recall_90: 0.7321\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 32s - loss: 0.6401 - accuracy: 0.6243 - precision_91: 0.6927 - recall_91: 0.2259\n",
      "Epoch 2/3\n",
      "215/215 - 31s - loss: 0.3937 - accuracy: 0.8276 - precision_91: 0.8491 - recall_91: 0.7283\n",
      "Epoch 3/3\n",
      "215/215 - 32s - loss: 0.2208 - accuracy: 0.9184 - precision_91: 0.9304 - recall_91: 0.8757\n",
      "215/215 - 5s - loss: 0.0979 - accuracy: 0.9707 - precision_91: 0.9841 - recall_91: 0.9470\n",
      "24/24 - 1s - loss: 0.4923 - accuracy: 0.7845 - precision_91: 0.7726 - recall_91: 0.7064\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 24s - loss: 0.6327 - accuracy: 0.6346 - precision_92: 0.7136 - recall_92: 0.2482\n",
      "Epoch 2/3\n",
      "215/215 - 23s - loss: 0.3825 - accuracy: 0.8399 - precision_92: 0.8602 - recall_92: 0.7487\n",
      "Epoch 3/3\n",
      "215/215 - 23s - loss: 0.2062 - accuracy: 0.9244 - precision_92: 0.9325 - recall_92: 0.8881\n",
      "215/215 - 4s - loss: 0.0826 - accuracy: 0.9730 - precision_92: 0.9845 - recall_92: 0.9521\n",
      "24/24 - 0s - loss: 0.6235 - accuracy: 0.7792 - precision_92: 0.7647 - recall_92: 0.7091\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 24s - loss: 0.6265 - accuracy: 0.6421 - precision_93: 0.7202 - recall_93: 0.2751\n",
      "Epoch 2/3\n",
      "215/215 - 23s - loss: 0.3786 - accuracy: 0.8393 - precision_93: 0.8612 - recall_93: 0.7469\n",
      "Epoch 3/3\n",
      "215/215 - 23s - loss: 0.2163 - accuracy: 0.9167 - precision_93: 0.9298 - recall_93: 0.8721\n",
      "215/215 - 5s - loss: 0.1017 - accuracy: 0.9691 - precision_93: 0.9797 - recall_93: 0.9478\n",
      "24/24 - 1s - loss: 0.5929 - accuracy: 0.7792 - precision_93: 0.7508 - recall_93: 0.7183\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "scores = []\n",
    "models = []\n",
    "\n",
    "i = 1\n",
    "for fold_train_indices, fold_val_indices in kfold.split(train_text, train_label):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i}')\n",
    "    print('------------------------------------------------------------------------')\n",
    "\n",
    "    fold_train_text = train_text[fold_train_indices]\n",
    "    fold_train_label = train_label[fold_train_indices]\n",
    "    fold_val_text = train_text[fold_val_indices]\n",
    "    fold_val_label = train_label[fold_val_indices]\n",
    "\n",
    "    model = build_model()\n",
    "    model.fit(fold_train_text, fold_train_label, epochs=epochs, verbose=2)\n",
    "    models.append(model)\n",
    "\n",
    "    fold_train_score = model.evaluate(fold_train_text, fold_train_label, verbose=2)\n",
    "    fold_val_score = model.evaluate(fold_val_text, fold_val_label, verbose=2)\n",
    "    scores.append({'train': fold_train_score, 'val': fold_val_score})\n",
    "\n",
    "    i += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "outputs": [],
   "source": [
    "for fold_scores in scores:\n",
    "    for subset in ['train', 'val']:\n",
    "        precision = fold_scores[subset][2]\n",
    "        recall = fold_scores[subset][3]\n",
    "        f1_score = 2/(1/precision + 1/recall)\n",
    "        fold_scores[subset].append(f1_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Train\n",
      ">>> Loss: 0.0934 - Accuracy: 0.968 - Precision: 0.9814 - Recall: 0.944 - F1-score: 0.9623\n",
      "> Fold 1 - Validation\n",
      ">>> Loss: 0.5739 - Accuracy: 0.7861 - Precision: 0.7466 - Recall: 0.7152 - F1-score: 0.7306\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Train\n",
      ">>> Loss: 0.0993 - Accuracy: 0.9658 - Precision: 0.9888 - Recall: 0.9312 - F1-score: 0.9592\n",
      "> Fold 2 - Validation\n",
      ">>> Loss: 0.4985 - Accuracy: 0.7992 - Precision: 0.8059 - Recall: 0.6875 - F1-score: 0.742\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Train\n",
      ">>> Loss: 0.0883 - Accuracy: 0.968 - Precision: 0.9789 - Recall: 0.9459 - F1-score: 0.9621\n",
      "> Fold 3 - Validation\n",
      ">>> Loss: 0.5747 - Accuracy: 0.7861 - Precision: 0.8 - Recall: 0.6767 - F1-score: 0.7332\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Train\n",
      ">>> Loss: 0.1105 - Accuracy: 0.9656 - Precision: 0.9761 - Recall: 0.9431 - F1-score: 0.9593\n",
      "> Fold 4 - Validation\n",
      ">>> Loss: 0.5641 - Accuracy: 0.7806 - Precision: 0.7831 - Recall: 0.6636 - F1-score: 0.7184\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Train\n",
      ">>> Loss: 0.099 - Accuracy: 0.973 - Precision: 0.9814 - Recall: 0.9549 - F1-score: 0.9679\n",
      "> Fold 5 - Validation\n",
      ">>> Loss: 0.5516 - Accuracy: 0.7674 - Precision: 0.77 - Recall: 0.6965 - F1-score: 0.7314\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Train\n",
      ">>> Loss: 0.0909 - Accuracy: 0.9723 - Precision: 0.9848 - Recall: 0.9498 - F1-score: 0.967\n",
      "> Fold 6 - Validation\n",
      ">>> Loss: 0.6019 - Accuracy: 0.7635 - Precision: 0.7726 - Recall: 0.6735 - F1-score: 0.7196\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Train\n",
      ">>> Loss: 0.1014 - Accuracy: 0.9712 - Precision: 0.9781 - Recall: 0.9546 - F1-score: 0.9662\n",
      "> Fold 7 - Validation\n",
      ">>> Loss: 0.5852 - Accuracy: 0.7608 - Precision: 0.71 - Recall: 0.7321 - F1-score: 0.7209\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Train\n",
      ">>> Loss: 0.0979 - Accuracy: 0.9707 - Precision: 0.9841 - Recall: 0.947 - F1-score: 0.9652\n",
      "> Fold 8 - Validation\n",
      ">>> Loss: 0.4923 - Accuracy: 0.7845 - Precision: 0.7726 - Recall: 0.7064 - F1-score: 0.738\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Train\n",
      ">>> Loss: 0.0826 - Accuracy: 0.973 - Precision: 0.9845 - Recall: 0.9521 - F1-score: 0.968\n",
      "> Fold 9 - Validation\n",
      ">>> Loss: 0.6235 - Accuracy: 0.7792 - Precision: 0.7647 - Recall: 0.7091 - F1-score: 0.7358\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Train\n",
      ">>> Loss: 0.1017 - Accuracy: 0.9691 - Precision: 0.9797 - Recall: 0.9478 - F1-score: 0.9634\n",
      "> Fold 10 - Validation\n",
      ">>> Loss: 0.5929 - Accuracy: 0.7792 - Precision: 0.7508 - Recall: 0.7183 - F1-score: 0.7342\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds - Train\n",
      "> Loss: 0.0965 -  Accuracy: 0.9697 - Precision: 0.9818 - Recall: 0.947 - F1-score: 0.9641\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds - Validation\n",
      "> Loss: 0.5659 -  Accuracy: 0.7787 - Precision: 0.7676 - Recall: 0.6979 - F1-score: 0.7304\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for fold_scores in scores:\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i} - Train')\n",
    "    print(f'>>> Loss: {round(fold_scores[\"train\"][0], 4)} - Accuracy: {round(fold_scores[\"train\"][1], 4)} - Precision: {round(fold_scores[\"train\"][2], 4)} - Recall: {round(fold_scores[\"train\"][3], 4)} - F1-score: {round(fold_scores[\"train\"][4], 4)}')\n",
    "    print(f'> Fold {i} - Validation')\n",
    "    print(f'>>> Loss: {round(fold_scores[\"val\"][0], 4)} - Accuracy: {round(fold_scores[\"val\"][1], 4)} - Precision: {round(fold_scores[\"val\"][2], 4)} - Recall: {round(fold_scores[\"val\"][3], 4)} - F1-score: {round(fold_scores[\"val\"][4], 4)}')\n",
    "    i += 1\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds - Train')\n",
    "print(f'> Loss: {round(np.mean([fold_score[\"train\"][0] for fold_score in scores]), 4)} -  Accuracy: {round(np.mean([fold_score[\"train\"][1] for fold_score in scores]), 4)} - Precision: {round(np.mean([fold_score[\"train\"][2] for fold_score in scores]), 4)} - Recall: {round(np.mean([fold_score[\"train\"][3] for fold_score in scores]), 4)} - F1-score: {round(np.mean([fold_score[\"train\"][4] for fold_score in scores]), 4)}')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds - Validation')\n",
    "print(f'> Loss: {round(np.mean([fold_score[\"val\"][0] for fold_score in scores]), 4)} -  Accuracy: {round(np.mean([fold_score[\"val\"][1] for fold_score in scores]), 4)} - Precision: {round(np.mean([fold_score[\"val\"][2] for fold_score in scores]), 4)} - Recall: {round(np.mean([fold_score[\"val\"][3] for fold_score in scores]), 4)} - F1-score: {round(np.mean([fold_score[\"val\"][4] for fold_score in scores]), 4)}')\n",
    "print('------------------------------------------------------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 - 7s - loss: 0.0064 - accuracy: 0.9959\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.006378879304975271, 0.9959279894828796]"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 1, 1, ..., 0, 1, 1])"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict(test_text)\n",
    "test_pred = np.round(test_pred).flatten().astype('int')\n",
    "\n",
    "test_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We generate vector counts for both train and test data using scikit's **CountVectorizer**. In particular, notice that we fit the vectorizer only with the train tokens, and use it to transform both train and test data. If there are N unique tokens in the train dataset, for each tweet we obtain a vector of length N whose values are the word counts:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "output = pd.DataFrame({'id': test_data['id'], 'target': test_pred})\n",
    "output.to_csv('predictions/nnets.csv', index=False)\n",
    "print(\"Submission successfully saved!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2021-09-13T15:13:20.767477Z",
     "iopub.execute_input": "2021-09-13T15:13:20.767825Z",
     "iopub.status.idle": "2021-09-13T15:13:20.800558Z",
     "shell.execute_reply.started": "2021-09-13T15:13:20.767795Z",
     "shell.execute_reply": "2021-09-13T15:13:20.799077Z"
    },
    "trusted": true
   },
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission successfully saved!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}