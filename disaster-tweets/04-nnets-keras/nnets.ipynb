{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 04 - Neural Networks with keras\n",
    "\n",
    "In this notebook we implement an approach based on neural networks, using the library **keras** from **tensorflow** to predict whether the tweets refer to a real disaster or not."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loading data\n",
    "\n",
    "We start by importing the packages we are going to use and loading the datasets:"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras.layers import TextVectorization\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "train_data = pd.read_csv(\"../data/train.csv\")\n",
    "test_data = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "train_data['text'].replace('http:\\/\\/\\S*', 'urltoken', regex=True, inplace=True)\n",
    "test_data['text'].replace('http:\\/\\/\\S*', 'urltoken', regex=True, inplace=True)\n",
    "\n",
    "train_text, train_label = np.array(train_data['text']), np.array(train_data['target'])\n",
    "test_text = test_data['text']\n",
    "\n",
    "print(train_text.shape)\n",
    "print(train_label.shape)\n",
    "print(test_text.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2021-09-13T15:08:54.835726Z",
     "iopub.execute_input": "2021-09-13T15:08:54.836899Z",
     "iopub.status.idle": "2021-09-13T15:08:54.893333Z",
     "shell.execute_reply.started": "2021-09-13T15:08:54.836857Z",
     "shell.execute_reply": "2021-09-13T15:08:54.89268Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613,)\n",
      "(7613,)\n",
      "(3263,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "count    7613.000000\nmean       14.903586\nstd         5.732604\nmin         1.000000\n25%        11.000000\n50%        15.000000\n75%        19.000000\nmax        31.000000\ndtype: float64"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word counts\n",
    "pd.Series(np.array([len(text.split()) for text in train_text])).describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "27736"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique tokens among all tweets\n",
    "len(np.unique(np.array(' '.join(train_text).split())))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "# Base model (overfitted)\n",
    "max_features = 20000\n",
    "sequence_length = 500\n",
    "\n",
    "embedding_dim = 128\n",
    "\n",
    "dropout_rate = 0.5\n",
    "\n",
    "conv_filters = 128\n",
    "\n",
    "conv_kernel_size = 7\n",
    "conv_strides = 3\n",
    "\n",
    "dense_layer_size = 128"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "# New base model after some tests (still overfitted)\n",
    "max_features = 10000\n",
    "sequence_length = 32\n",
    "\n",
    "embedding_dim = 64\n",
    "\n",
    "conv_filters = 64\n",
    "\n",
    "conv_kernel_size = 5\n",
    "conv_strides = 2\n",
    "\n",
    "dense_layer_size = 64"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "# New new base model after more tests\n",
    "max_features = 5000\n",
    "\n",
    "embedding_dim = 32\n",
    "\n",
    "conv_filters = 32\n",
    "\n",
    "conv_kernel_size = 3\n",
    "conv_strides = 1\n",
    "\n",
    "dense_layer_size = 32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [],
   "source": [
    "# Modifications\n",
    "# max_features = 2000\n",
    "\n",
    "embedding_dim = 16\n",
    "\n",
    "# conv_filters = 16\n",
    "\n",
    "# dense_layer_size = 16"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We clean the text by removing punctuation characters and stopwords:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "vectorizer = TextVectorization(\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "vectorizer.adapt(train_text)\n",
    "\n",
    "len(vectorizer.get_vocabulary())"
   ],
   "metadata": {},
   "execution_count": 182,
   "outputs": [
    {
     "data": {
      "text/plain": "5000"
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Inputs are text strings, then we vectorize them\n",
    "    inputs = keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
    "    x = vectorizer(inputs)\n",
    "\n",
    "    # We use Embedding to map the vectorized text onto a space of dimension embedding_dim\n",
    "    x = Embedding(max_features + 1, embedding_dim)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Conv1D + GlobalMaxPooling\n",
    "    x = Conv1D(conv_filters, conv_kernel_size, strides=conv_strides, activation='relu')(x)\n",
    "    x = Conv1D(conv_filters, conv_kernel_size, strides=conv_strides, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "    # Dense hidden layer\n",
    "    x = Dense(dense_layer_size, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "    model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_165\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text (InputLayer)            [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization_23 (TextV (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "embedding_165 (Embedding)    (None, 32, 16)            80016     \n",
      "_________________________________________________________________\n",
      "dropout_363 (Dropout)        (None, 32, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_330 (Conv1D)          (None, 30, 32)            1568      \n",
      "_________________________________________________________________\n",
      "conv1d_331 (Conv1D)          (None, 28, 32)            3104      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_165 (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_165 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_364 (Dropout)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 85,777\n",
      "Trainable params: 85,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "> Fold 1\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 3s - loss: 0.6770 - accuracy: 0.5712 - precision_166: 0.4775 - recall_166: 0.0181\n",
      "Epoch 2/3\n",
      "215/215 - 1s - loss: 0.6083 - accuracy: 0.6878 - precision_166: 0.7734 - recall_166: 0.3829\n",
      "Epoch 3/3\n",
      "215/215 - 1s - loss: 0.5107 - accuracy: 0.7764 - precision_166: 0.7706 - recall_166: 0.6802\n",
      "215/215 - 1s - loss: 0.3735 - accuracy: 0.8489 - precision_166: 0.8193 - recall_166: 0.8302\n",
      "24/24 - 0s - loss: 0.4835 - accuracy: 0.7822 - precision_166: 0.7402 - recall_166: 0.7840\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 3s - loss: 0.6814 - accuracy: 0.5671 - precision_167: 0.4674 - recall_167: 0.0145\n",
      "Epoch 2/3\n",
      "215/215 - 1s - loss: 0.5759 - accuracy: 0.7106 - precision_167: 0.7215 - recall_167: 0.5375\n",
      "Epoch 3/3\n",
      "215/215 - 1s - loss: 0.4412 - accuracy: 0.8140 - precision_167: 0.8388 - recall_167: 0.7051\n",
      "215/215 - 1s - loss: 0.3416 - accuracy: 0.8676 - precision_167: 0.8773 - recall_167: 0.8064\n",
      "24/24 - 0s - loss: 0.4598 - accuracy: 0.7966 - precision_167: 0.7635 - recall_167: 0.7267\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 2s - loss: 0.6786 - accuracy: 0.5685 - precision_168: 0.4194 - recall_168: 0.0088\n",
      "Epoch 2/3\n",
      "215/215 - 1s - loss: 0.5401 - accuracy: 0.7524 - precision_168: 0.7795 - recall_168: 0.5916\n",
      "Epoch 3/3\n",
      "215/215 - 1s - loss: 0.4207 - accuracy: 0.8263 - precision_168: 0.8385 - recall_168: 0.7383\n",
      "215/215 - 1s - loss: 0.3213 - accuracy: 0.8759 - precision_168: 0.9159 - recall_168: 0.7834\n",
      "24/24 - 0s - loss: 0.4329 - accuracy: 0.7992 - precision_168: 0.8071 - recall_168: 0.6954\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 3s - loss: 0.6701 - accuracy: 0.5921 - precision_169: 0.7508 - recall_169: 0.0767\n",
      "Epoch 2/3\n",
      "215/215 - 1s - loss: 0.5115 - accuracy: 0.7729 - precision_169: 0.7749 - recall_169: 0.6650\n",
      "Epoch 3/3\n",
      "215/215 - 1s - loss: 0.4126 - accuracy: 0.8272 - precision_169: 0.8433 - recall_169: 0.7346\n",
      "215/215 - 2s - loss: 0.3135 - accuracy: 0.8793 - precision_169: 0.8955 - recall_169: 0.8143\n",
      "24/24 - 0s - loss: 0.4538 - accuracy: 0.7871 - precision_169: 0.7604 - recall_169: 0.7323\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 3s - loss: 0.6784 - accuracy: 0.5706 - precision_170: 0.4507 - recall_170: 0.0109\n",
      "Epoch 2/3\n",
      "215/215 - 1s - loss: 0.5536 - accuracy: 0.7271 - precision_170: 0.7666 - recall_170: 0.5216\n",
      "Epoch 3/3\n",
      "215/215 - 1s - loss: 0.4300 - accuracy: 0.8187 - precision_170: 0.8358 - recall_170: 0.7179\n",
      "215/215 - 1s - loss: 0.3270 - accuracy: 0.8701 - precision_170: 0.8919 - recall_170: 0.7928\n",
      "24/24 - 0s - loss: 0.4195 - accuracy: 0.8187 - precision_170: 0.8214 - recall_170: 0.7530\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 2s - loss: 0.6824 - accuracy: 0.5674 - precision_171: 0.4762 - recall_171: 0.0034\n",
      "Epoch 2/3\n",
      "215/215 - 1s - loss: 0.5609 - accuracy: 0.7245 - precision_171: 0.7722 - recall_171: 0.5147\n",
      "Epoch 3/3\n",
      "215/215 - 1s - loss: 0.4483 - accuracy: 0.8164 - precision_171: 0.8329 - recall_171: 0.7199\n",
      "215/215 - 1s - loss: 0.3479 - accuracy: 0.8637 - precision_171: 0.9247 - recall_171: 0.7455\n",
      "24/24 - 0s - loss: 0.4638 - accuracy: 0.7963 - precision_171: 0.8148 - recall_171: 0.6429\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 2s - loss: 0.6814 - accuracy: 0.5665 - precision_172: 0.4595 - recall_172: 0.0540\n",
      "Epoch 2/3\n",
      "215/215 - 1s - loss: 0.5442 - accuracy: 0.7366 - precision_172: 0.7532 - recall_172: 0.5748\n",
      "Epoch 3/3\n",
      "215/215 - 1s - loss: 0.4277 - accuracy: 0.8189 - precision_172: 0.8316 - recall_172: 0.7250\n",
      "215/215 - 1s - loss: 0.3282 - accuracy: 0.8765 - precision_172: 0.8934 - recall_172: 0.8090\n",
      "24/24 - 0s - loss: 0.4641 - accuracy: 0.7806 - precision_172: 0.7755 - recall_172: 0.6930\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 2s - loss: 0.6835 - accuracy: 0.5651 - precision_173: 0.4639 - recall_173: 0.0829\n",
      "Epoch 2/3\n",
      "215/215 - 1s - loss: 0.5290 - accuracy: 0.7523 - precision_173: 0.7818 - recall_173: 0.5870\n",
      "Epoch 3/3\n",
      "215/215 - 1s - loss: 0.4145 - accuracy: 0.8284 - precision_173: 0.8447 - recall_173: 0.7356\n",
      "215/215 - 1s - loss: 0.3274 - accuracy: 0.8752 - precision_173: 0.8832 - recall_173: 0.8175\n",
      "24/24 - 0s - loss: 0.4770 - accuracy: 0.7753 - precision_173: 0.7633 - recall_173: 0.6960\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 2s - loss: 0.6785 - accuracy: 0.5782 - precision_174: 0.5826 - recall_174: 0.0635\n",
      "Epoch 2/3\n",
      "215/215 - 1s - loss: 0.5487 - accuracy: 0.7404 - precision_174: 0.7437 - recall_174: 0.6035\n",
      "Epoch 3/3\n",
      "215/215 - 1s - loss: 0.4278 - accuracy: 0.8203 - precision_174: 0.8333 - recall_174: 0.7271\n",
      "215/215 - 1s - loss: 0.3282 - accuracy: 0.8723 - precision_174: 0.9106 - recall_174: 0.7791\n",
      "24/24 - 0s - loss: 0.4319 - accuracy: 0.8213 - precision_174: 0.8453 - recall_174: 0.7165\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/3\n",
      "215/215 - 2s - loss: 0.6769 - accuracy: 0.5658 - precision_175: 0.3964 - recall_175: 0.0300\n",
      "Epoch 2/3\n",
      "215/215 - 1s - loss: 0.5481 - accuracy: 0.7374 - precision_175: 0.7606 - recall_175: 0.5630\n",
      "Epoch 3/3\n",
      "215/215 - 1s - loss: 0.4183 - accuracy: 0.8255 - precision_175: 0.8465 - recall_175: 0.7228\n",
      "215/215 - 1s - loss: 0.3161 - accuracy: 0.8803 - precision_175: 0.9064 - recall_175: 0.8030\n",
      "24/24 - 0s - loss: 0.4705 - accuracy: 0.7740 - precision_175: 0.7656 - recall_175: 0.7164\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "scores = []\n",
    "models = []\n",
    "\n",
    "i = 1\n",
    "for fold_train_indices, fold_val_indices in kfold.split(train_text, train_label):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i}')\n",
    "    print('------------------------------------------------------------------------')\n",
    "\n",
    "    fold_train_text = train_text[fold_train_indices]\n",
    "    fold_train_label = train_label[fold_train_indices]\n",
    "    fold_val_text = train_text[fold_val_indices]\n",
    "    fold_val_label = train_label[fold_val_indices]\n",
    "\n",
    "    model = build_model()\n",
    "    model.fit(fold_train_text, fold_train_label, epochs=epochs, verbose=2)\n",
    "    models.append(model)\n",
    "\n",
    "    fold_train_score = model.evaluate(fold_train_text, fold_train_label, verbose=2)\n",
    "    fold_val_score = model.evaluate(fold_val_text, fold_val_label, verbose=2)\n",
    "    scores.append({'train': fold_train_score, 'val': fold_val_score})\n",
    "\n",
    "    i += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "for fold_scores in scores:\n",
    "    for subset in ['train', 'val']:\n",
    "        precision = fold_scores[subset][2]\n",
    "        recall = fold_scores[subset][3]\n",
    "        f1_score = 2/(1/precision + 1/recall)\n",
    "        fold_scores[subset].append(f1_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Train\n",
      ">>> Loss: 0.3735 - Accuracy: 0.8489 - Precision: 0.8193 - Recall: 0.8302 - F1-score: 0.8247\n",
      "> Fold 1 - Validation\n",
      ">>> Loss: 0.4835 - Accuracy: 0.7822 - Precision: 0.7402 - Recall: 0.784 - F1-score: 0.7615\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Train\n",
      ">>> Loss: 0.3416 - Accuracy: 0.8676 - Precision: 0.8773 - Recall: 0.8064 - F1-score: 0.8403\n",
      "> Fold 2 - Validation\n",
      ">>> Loss: 0.4598 - Accuracy: 0.7966 - Precision: 0.7635 - Recall: 0.7267 - F1-score: 0.7446\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Train\n",
      ">>> Loss: 0.3213 - Accuracy: 0.8759 - Precision: 0.9159 - Recall: 0.7834 - F1-score: 0.8445\n",
      "> Fold 3 - Validation\n",
      ">>> Loss: 0.4329 - Accuracy: 0.7992 - Precision: 0.8071 - Recall: 0.6954 - F1-score: 0.7471\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Train\n",
      ">>> Loss: 0.3135 - Accuracy: 0.8793 - Precision: 0.8955 - Recall: 0.8143 - F1-score: 0.853\n",
      "> Fold 4 - Validation\n",
      ">>> Loss: 0.4538 - Accuracy: 0.7871 - Precision: 0.7604 - Recall: 0.7323 - F1-score: 0.7461\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Train\n",
      ">>> Loss: 0.327 - Accuracy: 0.8701 - Precision: 0.8919 - Recall: 0.7928 - F1-score: 0.8395\n",
      "> Fold 5 - Validation\n",
      ">>> Loss: 0.4195 - Accuracy: 0.8187 - Precision: 0.8214 - Recall: 0.753 - F1-score: 0.7857\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Train\n",
      ">>> Loss: 0.3479 - Accuracy: 0.8637 - Precision: 0.9247 - Recall: 0.7455 - F1-score: 0.8255\n",
      "> Fold 6 - Validation\n",
      ">>> Loss: 0.4638 - Accuracy: 0.7963 - Precision: 0.8148 - Recall: 0.6429 - F1-score: 0.7187\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Train\n",
      ">>> Loss: 0.3282 - Accuracy: 0.8765 - Precision: 0.8934 - Recall: 0.809 - F1-score: 0.8491\n",
      "> Fold 7 - Validation\n",
      ">>> Loss: 0.4641 - Accuracy: 0.7806 - Precision: 0.7755 - Recall: 0.693 - F1-score: 0.7319\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Train\n",
      ">>> Loss: 0.3274 - Accuracy: 0.8752 - Precision: 0.8832 - Recall: 0.8175 - F1-score: 0.8491\n",
      "> Fold 8 - Validation\n",
      ">>> Loss: 0.477 - Accuracy: 0.7753 - Precision: 0.7633 - Recall: 0.696 - F1-score: 0.7281\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Train\n",
      ">>> Loss: 0.3282 - Accuracy: 0.8723 - Precision: 0.9106 - Recall: 0.7791 - F1-score: 0.8398\n",
      "> Fold 9 - Validation\n",
      ">>> Loss: 0.4319 - Accuracy: 0.8213 - Precision: 0.8453 - Recall: 0.7165 - F1-score: 0.7756\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Train\n",
      ">>> Loss: 0.3161 - Accuracy: 0.8803 - Precision: 0.9064 - Recall: 0.803 - F1-score: 0.8516\n",
      "> Fold 10 - Validation\n",
      ">>> Loss: 0.4705 - Accuracy: 0.774 - Precision: 0.7656 - Recall: 0.7164 - F1-score: 0.7402\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds - Train\n",
      "> Loss: 0.3325 -  Accuracy: 0.871 - Precision: 0.8918 - Recall: 0.7981 - F1-score: 0.8417\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds - Validation\n",
      "> Loss: 0.4557 -  Accuracy: 0.7931 - Precision: 0.7857 - Recall: 0.7156 - F1-score: 0.748\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for fold_scores in scores:\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i} - Train')\n",
    "    print(f'>>> Loss: {round(fold_scores[\"train\"][0], 4)} - Accuracy: {round(fold_scores[\"train\"][1], 4)} - Precision: {round(fold_scores[\"train\"][2], 4)} - Recall: {round(fold_scores[\"train\"][3], 4)} - F1-score: {round(fold_scores[\"train\"][4], 4)}')\n",
    "    print(f'> Fold {i} - Validation')\n",
    "    print(f'>>> Loss: {round(fold_scores[\"val\"][0], 4)} - Accuracy: {round(fold_scores[\"val\"][1], 4)} - Precision: {round(fold_scores[\"val\"][2], 4)} - Recall: {round(fold_scores[\"val\"][3], 4)} - F1-score: {round(fold_scores[\"val\"][4], 4)}')\n",
    "    i += 1\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds - Train')\n",
    "print(f'> Loss: {round(np.mean([fold_score[\"train\"][0] for fold_score in scores]), 4)} -  Accuracy: {round(np.mean([fold_score[\"train\"][1] for fold_score in scores]), 4)} - Precision: {round(np.mean([fold_score[\"train\"][2] for fold_score in scores]), 4)} - Recall: {round(np.mean([fold_score[\"train\"][3] for fold_score in scores]), 4)} - F1-score: {round(np.mean([fold_score[\"train\"][4] for fold_score in scores]), 4)}')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds - Validation')\n",
    "print(f'> Loss: {round(np.mean([fold_score[\"val\"][0] for fold_score in scores]), 4)} -  Accuracy: {round(np.mean([fold_score[\"val\"][1] for fold_score in scores]), 4)} - Precision: {round(np.mean([fold_score[\"val\"][2] for fold_score in scores]), 4)} - Recall: {round(np.mean([fold_score[\"val\"][3] for fold_score in scores]), 4)} - F1-score: {round(np.mean([fold_score[\"val\"][4] for fold_score in scores]), 4)}')\n",
    "print('------------------------------------------------------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "238/238 - 3s - loss: 0.6748 - accuracy: 0.5698 - precision_176: 0.4861 - recall_176: 0.0214\n",
      "Epoch 2/3\n",
      "238/238 - 1s - loss: 0.5487 - accuracy: 0.7490 - precision_176: 0.7507 - recall_176: 0.6224\n",
      "Epoch 3/3\n",
      "238/238 - 1s - loss: 0.4255 - accuracy: 0.8236 - precision_176: 0.8423 - recall_176: 0.7252\n",
      "238/238 - 1s - loss: 0.3380 - accuracy: 0.8732 - precision_176: 0.8800 - recall_176: 0.8163\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.33802932500839233,\n 0.8732431530952454,\n 0.8800263404846191,\n 0.8162641525268555]"
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.fit(train_text, train_label, epochs=epochs, verbose=2)\n",
    "\n",
    "model.evaluate(train_text, train_label, verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 1, 1, ..., 1, 1, 1])"
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict(test_text)\n",
    "test_pred = np.round(test_pred).flatten().astype('int')\n",
    "\n",
    "test_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We generate vector counts for both train and test data using scikit's **CountVectorizer**. In particular, notice that we fit the vectorizer only with the train tokens, and use it to transform both train and test data. If there are N unique tokens in the train dataset, for each tweet we obtain a vector of length N whose values are the word counts:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "output = pd.DataFrame({'id': test_data['id'], 'target': test_pred})\n",
    "output.to_csv('predictions/nnets.csv', index=False)\n",
    "print(\"Submission successfully saved!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2021-09-13T15:13:20.767477Z",
     "iopub.execute_input": "2021-09-13T15:13:20.767825Z",
     "iopub.status.idle": "2021-09-13T15:13:20.800558Z",
     "shell.execute_reply.started": "2021-09-13T15:13:20.767795Z",
     "shell.execute_reply": "2021-09-13T15:13:20.799077Z"
    },
    "trusted": true
   },
   "execution_count": 190,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission successfully saved!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}