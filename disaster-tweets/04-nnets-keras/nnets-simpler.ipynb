{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 04 - Neural Networks with keras\n",
    "\n",
    "In this notebook we implement an approach based on neural networks, using the library **keras** from **tensorflow** to predict whether the tweets refer to a real disaster or not."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loading data\n",
    "\n",
    "We start by importing the packages we are going to use and loading the datasets:"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras.layers import TextVectorization\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "train_data = pd.read_csv(\"../data/train.csv\")\n",
    "test_data = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "train_data['text'].replace('http:\\/\\/\\S*', 'urltoken', regex=True, inplace=True)\n",
    "test_data['text'].replace('http:\\/\\/\\S*', 'urltoken', regex=True, inplace=True)\n",
    "\n",
    "train_text, train_label = np.array(train_data['text']), np.array(train_data['target'])\n",
    "test_text = test_data['text']\n",
    "\n",
    "print(train_text.shape)\n",
    "print(train_label.shape)\n",
    "print(test_text.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2021-09-13T15:08:54.835726Z",
     "iopub.execute_input": "2021-09-13T15:08:54.836899Z",
     "iopub.status.idle": "2021-09-13T15:08:54.893333Z",
     "shell.execute_reply.started": "2021-09-13T15:08:54.836857Z",
     "shell.execute_reply": "2021-09-13T15:08:54.89268Z"
    },
    "trusted": true
   },
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613,)\n",
      "(7613,)\n",
      "(3263,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "count    7613.000000\nmean       14.903586\nstd         5.732604\nmin         1.000000\n25%        11.000000\n50%        15.000000\n75%        19.000000\nmax        31.000000\ndtype: float64"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word counts\n",
    "pd.Series(np.array([len(text.split()) for text in train_text])).describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "data": {
      "text/plain": "27736"
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique tokens among all tweets\n",
    "len(np.unique(np.array(' '.join(train_text).split())))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We clean the text by removing punctuation characters and stopwords:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "max_features = 30000\n",
    "sequence_length = 32\n",
    "\n",
    "embedding_dim = 4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "vectorizer = TextVectorization(\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "vectorizer.adapt(train_text)\n",
    "\n",
    "len(vectorizer.get_vocabulary())"
   ],
   "metadata": {},
   "execution_count": 133,
   "outputs": [
    {
     "data": {
      "text/plain": "18510"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "c = tf.constant([\"i loved the movie yesterday, especially the part when the hero beat the monster after going through the maze filled of traps\", \"very concerned about the recent wildfires in California\"])\n",
    "c = vectorizer(c)\n",
    "c = Embedding(max_features + 1, embedding_dim)(c)\n",
    "c = Dense(embedding_dim, activation='relu')(c)\n",
    "c = GlobalMaxPooling1D()(c)\n",
    "c = Dense(1, activation='sigmoid', name='predictions')(c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Inputs are text strings, then we vectorize them\n",
    "    inputs = keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
    "    x = vectorizer(inputs)\n",
    "\n",
    "    # We use Embedding to map the vectorized text onto a space of dimension embedding_dim\n",
    "    x = Embedding(max_features + 1, embedding_dim)(x)\n",
    "\n",
    "    # Dense layer\n",
    "    x = Dense(embedding_dim, activation='relu')(x)\n",
    "\n",
    "    # GlobalMaxPooling\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(1, activation='sigmoid', name='predictions')(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text (InputLayer)            [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization_26 (TextV (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "embedding_65 (Embedding)     (None, 32, 4)             120004    \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 32, 4)             20        \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_52 (Glo (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 120,029\n",
      "Trainable params: 120,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "model = build_model()\n",
    "model.fit(train_text, train_label, epochs=epochs, verbose=2)\n",
    "\n",
    "model.evaluate(train_text, train_label, verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "> Fold 1\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/10\n",
      "215/215 - 3s - loss: 0.6783 - accuracy: 0.6085 - precision_44: 0.5817 - recall_44: 0.2979\n",
      "Epoch 2/10\n",
      "215/215 - 1s - loss: 0.6093 - accuracy: 0.7135 - precision_44: 0.8964 - recall_44: 0.3724\n",
      "Epoch 3/10\n",
      "215/215 - 1s - loss: 0.5006 - accuracy: 0.8021 - precision_44: 0.8611 - recall_44: 0.6399\n",
      "Epoch 4/10\n",
      "215/215 - 1s - loss: 0.4104 - accuracy: 0.8424 - precision_44: 0.8649 - recall_44: 0.7479\n",
      "Epoch 5/10\n",
      "215/215 - 1s - loss: 0.3458 - accuracy: 0.8692 - precision_44: 0.8813 - recall_44: 0.8018\n",
      "Epoch 6/10\n",
      "215/215 - 1s - loss: 0.2942 - accuracy: 0.8937 - precision_44: 0.8991 - recall_44: 0.8463\n",
      "Epoch 7/10\n",
      "215/215 - 1s - loss: 0.2516 - accuracy: 0.9130 - precision_44: 0.9152 - recall_44: 0.8777\n",
      "Epoch 8/10\n",
      "215/215 - 1s - loss: 0.2173 - accuracy: 0.9269 - precision_44: 0.9304 - recall_44: 0.8958\n",
      "Epoch 9/10\n",
      "215/215 - 1s - loss: 0.1881 - accuracy: 0.9385 - precision_44: 0.9396 - recall_44: 0.9149\n",
      "Epoch 10/10\n",
      "215/215 - 1s - loss: 0.1645 - accuracy: 0.9483 - precision_44: 0.9497 - recall_44: 0.9283\n",
      "215/215 - 1s - loss: 0.1411 - accuracy: 0.9584 - precision_44: 0.9577 - recall_44: 0.9443\n",
      "24/24 - 0s - loss: 0.5594 - accuracy: 0.7808 - precision_44: 0.7827 - recall_44: 0.7122\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/10\n",
      "215/215 - 2s - loss: 0.6851 - accuracy: 0.5606 - precision_45: 0.4673 - recall_45: 0.1579\n",
      "Epoch 2/10\n",
      "215/215 - 1s - loss: 0.6470 - accuracy: 0.6471 - precision_45: 0.8241 - recall_45: 0.2275\n",
      "Epoch 3/10\n",
      "215/215 - 1s - loss: 0.5717 - accuracy: 0.7381 - precision_45: 0.7825 - recall_45: 0.5413\n",
      "Epoch 4/10\n",
      "215/215 - 1s - loss: 0.4962 - accuracy: 0.7838 - precision_45: 0.8035 - recall_45: 0.6581\n",
      "Epoch 5/10\n",
      "215/215 - 1s - loss: 0.4272 - accuracy: 0.8234 - precision_45: 0.8343 - recall_45: 0.7351\n",
      "Epoch 6/10\n",
      "215/215 - 1s - loss: 0.3656 - accuracy: 0.8556 - precision_45: 0.8705 - recall_45: 0.7803\n",
      "Epoch 7/10\n",
      "215/215 - 1s - loss: 0.3148 - accuracy: 0.8791 - precision_45: 0.8936 - recall_45: 0.8160\n",
      "Epoch 8/10\n",
      "215/215 - 1s - loss: 0.2725 - accuracy: 0.8978 - precision_45: 0.9122 - recall_45: 0.8435\n",
      "Epoch 9/10\n",
      "215/215 - 1s - loss: 0.2375 - accuracy: 0.9177 - precision_45: 0.9302 - recall_45: 0.8740\n",
      "Epoch 10/10\n",
      "215/215 - 1s - loss: 0.2095 - accuracy: 0.9289 - precision_45: 0.9405 - recall_45: 0.8910\n",
      "215/215 - 1s - loss: 0.1826 - accuracy: 0.9432 - precision_45: 0.9513 - recall_45: 0.9148\n",
      "24/24 - 0s - loss: 0.5076 - accuracy: 0.7638 - precision_45: 0.7450 - recall_45: 0.6810\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/10\n",
      "215/215 - 2s - loss: 0.6690 - accuracy: 0.5720 - precision_46: 0.0000e+00 - recall_46: 0.0000e+00\n",
      "Epoch 2/10\n",
      "215/215 - 1s - loss: 0.6191 - accuracy: 0.6361 - precision_46: 0.9381 - recall_46: 0.1603\n",
      "Epoch 3/10\n",
      "215/215 - 1s - loss: 0.5741 - accuracy: 0.7878 - precision_46: 0.9307 - recall_46: 0.5447\n",
      "Epoch 4/10\n",
      "215/215 - 1s - loss: 0.5291 - accuracy: 0.8222 - precision_46: 0.9272 - recall_46: 0.6344\n",
      "Epoch 5/10\n",
      "215/215 - 1s - loss: 0.4850 - accuracy: 0.8390 - precision_46: 0.9220 - recall_46: 0.6814\n",
      "Epoch 6/10\n",
      "215/215 - 1s - loss: 0.4450 - accuracy: 0.8532 - precision_46: 0.9280 - recall_46: 0.7121\n",
      "Epoch 7/10\n",
      "215/215 - 1s - loss: 0.4122 - accuracy: 0.8583 - precision_46: 0.9302 - recall_46: 0.7231\n",
      "Epoch 8/10\n",
      "215/215 - 1s - loss: 0.3831 - accuracy: 0.8659 - precision_46: 0.9267 - recall_46: 0.7456\n",
      "Epoch 9/10\n",
      "215/215 - 1s - loss: 0.3589 - accuracy: 0.8716 - precision_46: 0.9271 - recall_46: 0.7595\n",
      "Epoch 10/10\n",
      "215/215 - 1s - loss: 0.3378 - accuracy: 0.8759 - precision_46: 0.9266 - recall_46: 0.7711\n",
      "215/215 - 1s - loss: 0.3177 - accuracy: 0.8826 - precision_46: 0.9350 - recall_46: 0.7800\n",
      "24/24 - 0s - loss: 0.6608 - accuracy: 0.6982 - precision_46: 0.6886 - recall_46: 0.5870\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/10\n",
      "215/215 - 2s - loss: 0.6834 - accuracy: 0.6169 - precision_47: 0.5789 - recall_47: 0.4041\n",
      "Epoch 2/10\n",
      "215/215 - 1s - loss: 0.6359 - accuracy: 0.7034 - precision_47: 0.6757 - recall_47: 0.5983\n",
      "Epoch 3/10\n",
      "215/215 - 1s - loss: 0.5503 - accuracy: 0.7577 - precision_47: 0.7384 - recall_47: 0.6773\n",
      "Epoch 4/10\n",
      "215/215 - 1s - loss: 0.4659 - accuracy: 0.8217 - precision_47: 0.8423 - recall_47: 0.7207\n",
      "Epoch 5/10\n",
      "215/215 - 1s - loss: 0.3991 - accuracy: 0.8433 - precision_47: 0.8550 - recall_47: 0.7658\n",
      "Epoch 6/10\n",
      "215/215 - 1s - loss: 0.3445 - accuracy: 0.8627 - precision_47: 0.8654 - recall_47: 0.8064\n",
      "Epoch 7/10\n",
      "215/215 - 1s - loss: 0.2991 - accuracy: 0.8843 - precision_47: 0.8829 - recall_47: 0.8431\n",
      "Epoch 8/10\n",
      "215/215 - 1s - loss: 0.2603 - accuracy: 0.9031 - precision_47: 0.8994 - recall_47: 0.8725\n",
      "Epoch 9/10\n",
      "215/215 - 1s - loss: 0.2271 - accuracy: 0.9194 - precision_47: 0.9169 - recall_47: 0.8939\n",
      "Epoch 10/10\n",
      "215/215 - 1s - loss: 0.1996 - accuracy: 0.9316 - precision_47: 0.9291 - recall_47: 0.9105\n",
      "215/215 - 1s - loss: 0.1745 - accuracy: 0.9432 - precision_47: 0.9396 - recall_47: 0.9278\n",
      "24/24 - 0s - loss: 0.5042 - accuracy: 0.7727 - precision_47: 0.7741 - recall_47: 0.6511\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/10\n",
      "215/215 - 3s - loss: 0.6681 - accuracy: 0.5725 - precision_48: 0.8421 - recall_48: 0.0054\n",
      "Epoch 2/10\n",
      "215/215 - 1s - loss: 0.6064 - accuracy: 0.7196 - precision_48: 0.8681 - recall_48: 0.4092\n",
      "Epoch 3/10\n",
      "215/215 - 1s - loss: 0.5341 - accuracy: 0.8114 - precision_48: 0.8443 - recall_48: 0.6876\n",
      "Epoch 4/10\n",
      "215/215 - 1s - loss: 0.4590 - accuracy: 0.8396 - precision_48: 0.8717 - recall_48: 0.7345\n",
      "Epoch 5/10\n",
      "215/215 - 1s - loss: 0.3939 - accuracy: 0.8581 - precision_48: 0.8881 - recall_48: 0.7661\n",
      "Epoch 6/10\n",
      "215/215 - 1s - loss: 0.3424 - accuracy: 0.8741 - precision_48: 0.9012 - recall_48: 0.7937\n",
      "Epoch 7/10\n",
      "215/215 - 1s - loss: 0.2994 - accuracy: 0.8926 - precision_48: 0.9150 - recall_48: 0.8266\n",
      "Epoch 8/10\n",
      "215/215 - 1s - loss: 0.2661 - accuracy: 0.9065 - precision_48: 0.9314 - recall_48: 0.8443\n",
      "Epoch 9/10\n",
      "215/215 - 1s - loss: 0.2379 - accuracy: 0.9140 - precision_48: 0.9321 - recall_48: 0.8627\n",
      "Epoch 10/10\n",
      "215/215 - 1s - loss: 0.2149 - accuracy: 0.9225 - precision_48: 0.9411 - recall_48: 0.8742\n",
      "215/215 - 1s - loss: 0.1902 - accuracy: 0.9353 - precision_48: 0.9549 - recall_48: 0.8916\n",
      "24/24 - 0s - loss: 0.5429 - accuracy: 0.7714 - precision_48: 0.7341 - recall_48: 0.7386\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/10\n",
      "215/215 - 2s - loss: 0.6800 - accuracy: 0.5981 - precision_49: 0.5768 - recall_49: 0.2495\n",
      "Epoch 2/10\n",
      "215/215 - 1s - loss: 0.6292 - accuracy: 0.6967 - precision_49: 0.8049 - recall_49: 0.3902\n",
      "Epoch 3/10\n",
      "215/215 - 1s - loss: 0.5463 - accuracy: 0.7539 - precision_49: 0.8077 - recall_49: 0.5624\n",
      "Epoch 4/10\n",
      "215/215 - 1s - loss: 0.4720 - accuracy: 0.7897 - precision_49: 0.8201 - recall_49: 0.6553\n",
      "Epoch 5/10\n",
      "215/215 - 1s - loss: 0.4117 - accuracy: 0.8265 - precision_49: 0.8342 - recall_49: 0.7451\n",
      "Epoch 6/10\n",
      "215/215 - 1s - loss: 0.3612 - accuracy: 0.8596 - precision_49: 0.8423 - recall_49: 0.8292\n",
      "Epoch 7/10\n",
      "215/215 - 1s - loss: 0.3215 - accuracy: 0.8729 - precision_49: 0.8485 - recall_49: 0.8580\n",
      "Epoch 8/10\n",
      "215/215 - 1s - loss: 0.2921 - accuracy: 0.8795 - precision_49: 0.8453 - recall_49: 0.8814\n",
      "Epoch 9/10\n",
      "215/215 - 1s - loss: 0.2718 - accuracy: 0.8837 - precision_49: 0.8519 - recall_49: 0.8834\n",
      "Epoch 10/10\n",
      "215/215 - 1s - loss: 0.2575 - accuracy: 0.8860 - precision_49: 0.8491 - recall_49: 0.8942\n",
      "215/215 - 1s - loss: 0.2395 - accuracy: 0.8935 - precision_49: 0.8656 - recall_49: 0.8908\n",
      "24/24 - 0s - loss: 0.5272 - accuracy: 0.7766 - precision_49: 0.7525 - recall_49: 0.7009\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/10\n",
      "215/215 - 2s - loss: 0.6809 - accuracy: 0.6470 - precision_50: 0.5891 - recall_50: 0.5716\n",
      "Epoch 2/10\n",
      "215/215 - 1s - loss: 0.6179 - accuracy: 0.7931 - precision_50: 0.7910 - recall_50: 0.7002\n",
      "Epoch 3/10\n",
      "215/215 - 1s - loss: 0.5387 - accuracy: 0.8338 - precision_50: 0.8114 - recall_50: 0.7956\n",
      "Epoch 4/10\n",
      "215/215 - 1s - loss: 0.4743 - accuracy: 0.8590 - precision_50: 0.8288 - recall_50: 0.8441\n",
      "Epoch 5/10\n",
      "215/215 - 1s - loss: 0.4214 - accuracy: 0.8768 - precision_50: 0.8486 - recall_50: 0.8660\n",
      "Epoch 6/10\n",
      "215/215 - 1s - loss: 0.3769 - accuracy: 0.8962 - precision_50: 0.8794 - recall_50: 0.8773\n",
      "Epoch 7/10\n",
      "215/215 - 1s - loss: 0.3390 - accuracy: 0.9060 - precision_50: 0.8875 - recall_50: 0.8930\n",
      "Epoch 8/10\n",
      "215/215 - 1s - loss: 0.3068 - accuracy: 0.9149 - precision_50: 0.8994 - recall_50: 0.9015\n",
      "Epoch 9/10\n",
      "215/215 - 1s - loss: 0.2789 - accuracy: 0.9225 - precision_50: 0.9083 - recall_50: 0.9104\n",
      "Epoch 10/10\n",
      "215/215 - 1s - loss: 0.2564 - accuracy: 0.9276 - precision_50: 0.9195 - recall_50: 0.9101\n",
      "215/215 - 1s - loss: 0.2371 - accuracy: 0.9359 - precision_50: 0.9352 - recall_50: 0.9132\n",
      "24/24 - 0s - loss: 0.5383 - accuracy: 0.7543 - precision_50: 0.7508 - recall_50: 0.6879\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/10\n",
      "215/215 - 2s - loss: 0.6646 - accuracy: 0.5838 - precision_51: 0.8413 - recall_51: 0.0361\n",
      "Epoch 2/10\n",
      "215/215 - 1s - loss: 0.5867 - accuracy: 0.7237 - precision_51: 0.8955 - recall_51: 0.4027\n",
      "Epoch 3/10\n",
      "215/215 - 1s - loss: 0.4969 - accuracy: 0.8152 - precision_51: 0.8647 - recall_51: 0.6746\n",
      "Epoch 4/10\n",
      "215/215 - 1s - loss: 0.4289 - accuracy: 0.8457 - precision_51: 0.8866 - recall_51: 0.7342\n",
      "Epoch 5/10\n",
      "215/215 - 1s - loss: 0.3754 - accuracy: 0.8651 - precision_51: 0.8924 - recall_51: 0.7794\n",
      "Epoch 6/10\n",
      "215/215 - 1s - loss: 0.3310 - accuracy: 0.8767 - precision_51: 0.8975 - recall_51: 0.8043\n",
      "Epoch 7/10\n",
      "215/215 - 1s - loss: 0.2923 - accuracy: 0.8911 - precision_51: 0.9087 - recall_51: 0.8295\n",
      "Epoch 8/10\n",
      "215/215 - 1s - loss: 0.2589 - accuracy: 0.9053 - precision_51: 0.9200 - recall_51: 0.8533\n",
      "Epoch 9/10\n",
      "215/215 - 1s - loss: 0.2302 - accuracy: 0.9154 - precision_51: 0.9322 - recall_51: 0.8656\n",
      "Epoch 10/10\n",
      "215/215 - 1s - loss: 0.2064 - accuracy: 0.9250 - precision_51: 0.9341 - recall_51: 0.8877\n",
      "215/215 - 1s - loss: 0.1826 - accuracy: 0.9371 - precision_51: 0.9560 - recall_51: 0.8945\n",
      "24/24 - 0s - loss: 0.5484 - accuracy: 0.7582 - precision_51: 0.7560 - recall_51: 0.6607\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/10\n",
      "215/215 - 2s - loss: 0.6733 - accuracy: 0.5852 - precision_52: 0.8027 - recall_52: 0.0601\n",
      "Epoch 2/10\n",
      "215/215 - 1s - loss: 0.6061 - accuracy: 0.7284 - precision_52: 0.7339 - recall_52: 0.5882\n",
      "Epoch 3/10\n",
      "215/215 - 1s - loss: 0.5129 - accuracy: 0.7840 - precision_52: 0.7847 - recall_52: 0.6930\n",
      "Epoch 4/10\n",
      "215/215 - 1s - loss: 0.4319 - accuracy: 0.8376 - precision_52: 0.8687 - recall_52: 0.7377\n",
      "Epoch 5/10\n",
      "215/215 - 1s - loss: 0.3691 - accuracy: 0.8625 - precision_52: 0.8900 - recall_52: 0.7800\n",
      "Epoch 6/10\n",
      "215/215 - 1s - loss: 0.3173 - accuracy: 0.8830 - precision_52: 0.9032 - recall_52: 0.8183\n",
      "Epoch 7/10\n",
      "215/215 - 1s - loss: 0.2740 - accuracy: 0.8973 - precision_52: 0.9131 - recall_52: 0.8438\n",
      "Epoch 8/10\n",
      "215/215 - 1s - loss: 0.2374 - accuracy: 0.9162 - precision_52: 0.9277 - recall_52: 0.8754\n",
      "Epoch 9/10\n",
      "215/215 - 1s - loss: 0.2061 - accuracy: 0.9281 - precision_52: 0.9376 - recall_52: 0.8939\n",
      "Epoch 10/10\n",
      "215/215 - 1s - loss: 0.1806 - accuracy: 0.9390 - precision_52: 0.9507 - recall_52: 0.9066\n",
      "215/215 - 1s - loss: 0.1570 - accuracy: 0.9489 - precision_52: 0.9543 - recall_52: 0.9268\n",
      "24/24 - 0s - loss: 0.5328 - accuracy: 0.7858 - precision_52: 0.7282 - recall_52: 0.7109\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10\n",
      "------------------------------------------------------------------------\n",
      "Epoch 1/10\n",
      "215/215 - 2s - loss: 0.6676 - accuracy: 0.5787 - precision_53: 0.9024 - recall_53: 0.0251\n",
      "Epoch 2/10\n",
      "215/215 - 1s - loss: 0.6111 - accuracy: 0.7148 - precision_53: 0.8606 - recall_53: 0.4037\n",
      "Epoch 3/10\n",
      "215/215 - 1s - loss: 0.5217 - accuracy: 0.8148 - precision_53: 0.8526 - recall_53: 0.6895\n",
      "Epoch 4/10\n",
      "215/215 - 1s - loss: 0.4384 - accuracy: 0.8462 - precision_53: 0.8719 - recall_53: 0.7538\n",
      "Epoch 5/10\n",
      "215/215 - 1s - loss: 0.3783 - accuracy: 0.8653 - precision_53: 0.8949 - recall_53: 0.7789\n",
      "Epoch 6/10\n",
      "215/215 - 1s - loss: 0.3356 - accuracy: 0.8746 - precision_53: 0.8954 - recall_53: 0.8029\n",
      "Epoch 7/10\n",
      "215/215 - 1s - loss: 0.3032 - accuracy: 0.8827 - precision_53: 0.9081 - recall_53: 0.8097\n",
      "Epoch 8/10\n",
      "215/215 - 1s - loss: 0.2778 - accuracy: 0.8904 - precision_53: 0.9105 - recall_53: 0.8270\n",
      "Epoch 9/10\n",
      "215/215 - 1s - loss: 0.2573 - accuracy: 0.8981 - precision_53: 0.9177 - recall_53: 0.8388\n",
      "Epoch 10/10\n",
      "215/215 - 1s - loss: 0.2401 - accuracy: 0.9040 - precision_53: 0.9189 - recall_53: 0.8524\n",
      "215/215 - 1s - loss: 0.2171 - accuracy: 0.9168 - precision_53: 0.9344 - recall_53: 0.8679\n",
      "24/24 - 0s - loss: 0.5983 - accuracy: 0.7306 - precision_53: 0.6687 - recall_53: 0.7044\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "scores = []\n",
    "models = []\n",
    "\n",
    "i = 1\n",
    "for fold_train_indices, fold_val_indices in kfold.split(train_text, train_label):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i}')\n",
    "    print('------------------------------------------------------------------------')\n",
    "\n",
    "    fold_train_text = train_text[fold_train_indices]\n",
    "    fold_train_label = train_label[fold_train_indices]\n",
    "    fold_val_text = train_text[fold_val_indices]\n",
    "    fold_val_label = train_label[fold_val_indices]\n",
    "\n",
    "    model = build_model()\n",
    "    model.fit(fold_train_text, fold_train_label, epochs=epochs, verbose=2)\n",
    "    models.append(model)\n",
    "\n",
    "    fold_train_score = model.evaluate(fold_train_text, fold_train_label, verbose=2)\n",
    "    fold_val_score = model.evaluate(fold_val_text, fold_val_label, verbose=2)\n",
    "    scores.append({'train': fold_train_score, 'val': fold_val_score})\n",
    "\n",
    "    i += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "for fold_scores in scores:\n",
    "    for subset in ['train', 'val']:\n",
    "        precision = fold_scores[subset][2]\n",
    "        recall = fold_scores[subset][3]\n",
    "        f1_score = 2/(1/precision + 1/recall)\n",
    "        fold_scores[subset].append(f1_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Average scores for all folds - Train\n",
      "> Loss: 0.2039 -  Accuracy: 0.9295 - Precision: 0.9384 - Recall: 0.8952 - F1-score: 0.9157\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds - Validation\n",
      "> Loss: 0.552 -  Accuracy: 0.7592 - Precision: 0.7381 - Recall: 0.6835 - F1-score: 0.7089\n",
      "------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Train\n",
      ">>> Loss: 0.1411 - Accuracy: 0.9584 - Precision: 0.9577 - Recall: 0.9443 - F1-score: 0.951\n",
      "> Fold 1 - Validation\n",
      ">>> Loss: 0.5594 - Accuracy: 0.7808 - Precision: 0.7827 - Recall: 0.7122 - F1-score: 0.7458\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Train\n",
      ">>> Loss: 0.1826 - Accuracy: 0.9432 - Precision: 0.9513 - Recall: 0.9148 - F1-score: 0.9327\n",
      "> Fold 2 - Validation\n",
      ">>> Loss: 0.5076 - Accuracy: 0.7638 - Precision: 0.745 - Recall: 0.681 - F1-score: 0.7115\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Train\n",
      ">>> Loss: 0.3177 - Accuracy: 0.8826 - Precision: 0.935 - Recall: 0.78 - F1-score: 0.8505\n",
      "> Fold 3 - Validation\n",
      ">>> Loss: 0.6608 - Accuracy: 0.6982 - Precision: 0.6886 - Recall: 0.587 - F1-score: 0.6338\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Train\n",
      ">>> Loss: 0.1745 - Accuracy: 0.9432 - Precision: 0.9396 - Recall: 0.9278 - F1-score: 0.9337\n",
      "> Fold 4 - Validation\n",
      ">>> Loss: 0.5042 - Accuracy: 0.7727 - Precision: 0.7741 - Recall: 0.6511 - F1-score: 0.7073\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Train\n",
      ">>> Loss: 0.1902 - Accuracy: 0.9353 - Precision: 0.9549 - Recall: 0.8916 - F1-score: 0.9221\n",
      "> Fold 5 - Validation\n",
      ">>> Loss: 0.5429 - Accuracy: 0.7714 - Precision: 0.7341 - Recall: 0.7386 - F1-score: 0.7364\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Train\n",
      ">>> Loss: 0.2395 - Accuracy: 0.8935 - Precision: 0.8656 - Recall: 0.8908 - F1-score: 0.878\n",
      "> Fold 6 - Validation\n",
      ">>> Loss: 0.5272 - Accuracy: 0.7766 - Precision: 0.7525 - Recall: 0.7009 - F1-score: 0.7258\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Train\n",
      ">>> Loss: 0.2371 - Accuracy: 0.9359 - Precision: 0.9352 - Recall: 0.9132 - F1-score: 0.9241\n",
      "> Fold 7 - Validation\n",
      ">>> Loss: 0.5383 - Accuracy: 0.7543 - Precision: 0.7508 - Recall: 0.6879 - F1-score: 0.7179\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Train\n",
      ">>> Loss: 0.1826 - Accuracy: 0.9371 - Precision: 0.956 - Recall: 0.8945 - F1-score: 0.9242\n",
      "> Fold 8 - Validation\n",
      ">>> Loss: 0.5484 - Accuracy: 0.7582 - Precision: 0.756 - Recall: 0.6607 - F1-score: 0.7051\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Train\n",
      ">>> Loss: 0.157 - Accuracy: 0.9489 - Precision: 0.9543 - Recall: 0.9268 - F1-score: 0.9404\n",
      "> Fold 9 - Validation\n",
      ">>> Loss: 0.5328 - Accuracy: 0.7858 - Precision: 0.7282 - Recall: 0.7109 - F1-score: 0.7194\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Train\n",
      ">>> Loss: 0.2171 - Accuracy: 0.9168 - Precision: 0.9344 - Recall: 0.8679 - F1-score: 0.8999\n",
      "> Fold 10 - Validation\n",
      ">>> Loss: 0.5983 - Accuracy: 0.7306 - Precision: 0.6687 - Recall: 0.7044 - F1-score: 0.6861\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds - Train')\n",
    "print(f'> Loss: {round(np.mean([fold_score[\"train\"][0] for fold_score in scores]), 4)} -  Accuracy: {round(np.mean([fold_score[\"train\"][1] for fold_score in scores]), 4)} - Precision: {round(np.mean([fold_score[\"train\"][2] for fold_score in scores]), 4)} - Recall: {round(np.mean([fold_score[\"train\"][3] for fold_score in scores]), 4)} - F1-score: {round(np.mean([fold_score[\"train\"][4] for fold_score in scores]), 4)}')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds - Validation')\n",
    "print(f'> Loss: {round(np.mean([fold_score[\"val\"][0] for fold_score in scores]), 4)} -  Accuracy: {round(np.mean([fold_score[\"val\"][1] for fold_score in scores]), 4)} - Precision: {round(np.mean([fold_score[\"val\"][2] for fold_score in scores]), 4)} - Recall: {round(np.mean([fold_score[\"val\"][3] for fold_score in scores]), 4)} - F1-score: {round(np.mean([fold_score[\"val\"][4] for fold_score in scores]), 4)}')\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "i = 1\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for fold_scores in scores:\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i} - Train')\n",
    "    print(f'>>> Loss: {round(fold_scores[\"train\"][0], 4)} - Accuracy: {round(fold_scores[\"train\"][1], 4)} - Precision: {round(fold_scores[\"train\"][2], 4)} - Recall: {round(fold_scores[\"train\"][3], 4)} - F1-score: {round(fold_scores[\"train\"][4], 4)}')\n",
    "    print(f'> Fold {i} - Validation')\n",
    "    print(f'>>> Loss: {round(fold_scores[\"val\"][0], 4)} - Accuracy: {round(fold_scores[\"val\"][1], 4)} - Precision: {round(fold_scores[\"val\"][2], 4)} - Recall: {round(fold_scores[\"val\"][3], 4)} - F1-score: {round(fold_scores[\"val\"][4], 4)}')\n",
    "    i += 1\n",
    "print('------------------------------------------------------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "model = models[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 1, 1, ..., 1, 1, 1])"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict(test_text)\n",
    "test_pred = np.round(test_pred).flatten().astype('int')\n",
    "\n",
    "test_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We generate vector counts for both train and test data using scikit's **CountVectorizer**. In particular, notice that we fit the vectorizer only with the train tokens, and use it to transform both train and test data. If there are N unique tokens in the train dataset, for each tweet we obtain a vector of length N whose values are the word counts:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "output = pd.DataFrame({'id': test_data['id'], 'target': test_pred})\n",
    "output.to_csv('predictions/nnets.csv', index=False)\n",
    "print(\"Submission successfully saved!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2021-09-13T15:13:20.767477Z",
     "iopub.execute_input": "2021-09-13T15:13:20.767825Z",
     "iopub.status.idle": "2021-09-13T15:13:20.800558Z",
     "shell.execute_reply.started": "2021-09-13T15:13:20.767795Z",
     "shell.execute_reply": "2021-09-13T15:13:20.799077Z"
    },
    "trusted": true
   },
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission successfully saved!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}